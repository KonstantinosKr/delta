#include "tarch/compiler/CompilerSpecificSettings.h"


template<class Data, bool CreateCopiesOfSentData>
tarch::logging::Log  peano::heap::SynchronousDataExchanger<Data, CreateCopiesOfSentData>::_log( "peano::heap::SynchronousDataExchanger" );


template<class Data, bool CreateCopiesOfSentData>
peano::heap::SynchronousDataExchanger<Data, CreateCopiesOfSentData>::SynchronousDataExchanger(
  const std::string& identifier,
  int metaDataTag,
  int dataTag
):
  _identifier(identifier),
  _metaDataTag(metaDataTag),
  _dataTag(dataTag),
  _sendTasks(),
  _receiveTasks(),
  _numberOfSentMessages(0),
  _numberOfSentRecords(0),
  _numberOfReceivedMessages(0),
  _numberOfReceivedRecords(0)
  #ifdef Asserts
  ,_isCurrentlySending(false)
  #endif
  {
}


template<class Data, bool CreateCopiesOfSentData>
void peano::heap::SynchronousDataExchanger<Data, CreateCopiesOfSentData>::startToSendData() {
  logTraceInWith1Argument( "startToSendData()", _identifier );
  assertion( _sendTasks.empty() );

  #ifdef Asserts
  _isCurrentlySending = true;
  #endif

  logTraceOut( "startToSendData()" );
}


template<class Data, bool CreateCopiesOfSentData>
void peano::heap::SynchronousDataExchanger<Data, CreateCopiesOfSentData>::finishedToSendData() {
  logTraceInWith1Argument( "finishedToSendData()", _sendTasks.size() );

  #ifdef Asserts
  _isCurrentlySending = false;
  #endif

  for(typename std::list<SendReceiveTask<Data> >::iterator i = _sendTasks.begin(); i != _sendTasks.end(); ++i) {
    MPI_Status status;

    const clock_t  timeOutWarning          = tarch::parallel::Node::getInstance().getDeadlockWarningTimeStamp();
    const clock_t  timeOutShutdown         = tarch::parallel::Node::getInstance().getDeadlockTimeOutTimeStamp();
    bool           triggeredTimeoutWarning = false;
    int            finishedWait            = i->_metaInformation.getLength()==0;

    while (!finishedWait) {
      MPI_Test(&(i->_request), &finishedWait, &status);

      // deadlock aspect
      if (
         tarch::parallel::Node::getInstance().isTimeOutWarningEnabled() &&
         (clock()>timeOutWarning) &&
         (!triggeredTimeoutWarning)
      ) {
         tarch::parallel::Node::getInstance().writeTimeOutWarning(
           "peano::heap::SynchronousDataExchanger",
           "finishedToSendData()", i->_rank,-1,i->_metaInformation.getLength()
         );
         triggeredTimeoutWarning = true;
      }
      if (
         tarch::parallel::Node::getInstance().isTimeOutDeadlockEnabled() &&
         (clock()>timeOutShutdown)
      ) {
        std::ostringstream msg;
        msg << "meta-data-tag=" << _metaDataTag;
        msg << ",data-tag=" << _dataTag;
        tarch::parallel::Node::getInstance().triggerDeadlockTimeOut(
           "peano::heap::SynchronousDataExchanger",
           "finishedToSendData()", i->_rank,
           -1,
           i->_metaInformation.getLength(), msg.str()
        );
      }
      tarch::parallel::Node::getInstance().receiveDanglingMessages();
    }

    i->freeMemoryOfSendTask();
  }

  _sendTasks.clear();

  logTraceOut( "finishedToSendData()" );
}


template<class Data, bool CreateCopiesOfSentData>
void peano::heap::SynchronousDataExchanger<Data, CreateCopiesOfSentData>::receiveDanglingMessages() {
  int        flag   = 0;
  MPI_Status status;
  int        result = MPI_Iprobe(
    MPI_ANY_SOURCE,
    _metaDataTag,
    tarch::parallel::Node::getInstance().getCommunicator(),
    &flag,
    &status
  );
  if (result!=MPI_SUCCESS) {
    logError(
      "receiveDanglingMessages()",
      "probing for messages failed: " << tarch::parallel::MPIReturnValueToString(result)
    );
  }
  if (flag) {
    logTraceInWith1Argument( "receiveDanglingMessages(...)", _tag );

    SendReceiveTask<Data> receiveTask;

    // @todo Has to be -1 (we may not call receiveDangling in-between as this would mess up the order of the real data). Documentation!
    receiveTask._metaInformation.receive(status.MPI_SOURCE, _metaDataTag, true, -1);
    receiveTask._rank = status.MPI_SOURCE;

    _numberOfReceivedMessages += 1;
    _numberOfSentRecords      += receiveTask._metaInformation.getLength();

    _receiveTasks.push_back( receiveTask );

    if(receiveTask._metaInformation.getLength() > 0) {
      _receiveTasks.back().triggerReceive(_dataTag);
      logDebug(
        "receiveDanglingMessages(...)",
        "started to receive " << _receiveTasks.size() << "th message from rank " << receiveTask._rank << " with " << receiveTask._metaInformation.getLength() << " entries and data pointing to " << receiveTask._data
      );
    }

    logTraceOut( "receiveDanglingMessages(...)" );
  }
}


template<class Data, bool CreateCopiesOfSentData>
void peano::heap::SynchronousDataExchanger<Data, CreateCopiesOfSentData>::sendData(
  const std::vector<Data>&                      data,
  int                                           toRank,
  const tarch::la::Vector<DIMENSIONS, double>&  position,
  int                                           level
) {
  assertion( _isCurrentlySending );

  SendReceiveTask<Data> sendTask;
  sendTask._rank = toRank;
  #ifdef Asserts
  //Set debug information
  sendTask._metaInformation.setPosition(position);
  sendTask._metaInformation.setLevel(level);
  #endif

  logDebug("sendData", "Sending data at " << position << " to Rank " << toRank << " with tag " << _tag  );

  sendTask._metaInformation.setLength(static_cast<int>( data.size() ));
  // @todo See comment above on the -1
  sendTask._metaInformation.send(toRank, _metaDataTag, true, -1 );
  _sendTasks.push_back(sendTask);

  if(data.size() > 0) {
    if (CreateCopiesOfSentData) {
      _sendTasks.back().wrapData(data);
    }
    else {
      _sendTasks.back().sendDataDirectlyFromBuffer(data);
    }

    _sendTasks.back().triggerSend(_dataTag);
  }

  _numberOfSentMessages += 1;
  _numberOfSentRecords  += data.size();
}


template<class Data, bool CreateCopiesOfSentData>
std::vector< Data > peano::heap::SynchronousDataExchanger<Data, CreateCopiesOfSentData>::receiveData(
  int                                           fromRank,
  const tarch::la::Vector<DIMENSIONS, double>&  position,
  int                                           level
) {
  logTraceInWith3Arguments( "receiveData(...)", fromRank, position, level );

  std::vector< Data >  result;

  typename std::list< SendReceiveTask<Data> >::iterator resultTask = findMessageFromRankInReceiveBuffer(fromRank);

  if (resultTask == _receiveTasks.end()) {
    logDebug( "receiveData", "data either not found or records not available yet" );

    const clock_t  timeOutWarning          = tarch::parallel::Node::getInstance().getDeadlockWarningTimeStamp();
    const clock_t  timeOutShutdown         = tarch::parallel::Node::getInstance().getDeadlockTimeOutTimeStamp();
    bool           triggeredTimeoutWarning = false;

    while (resultTask == _receiveTasks.end()) {
      // deadlock aspect
      if (
         tarch::parallel::Node::getInstance().isTimeOutWarningEnabled() &&
         (clock()>timeOutWarning) &&
         (!triggeredTimeoutWarning)
      ) {
         tarch::parallel::Node::getInstance().writeTimeOutWarning(
           "peano::heap::SynchronousDataExchanger",
           "receiveData()", fromRank,
           _metaDataTag, -1
         );
         triggeredTimeoutWarning = true;
      }
      if (
         tarch::parallel::Node::getInstance().isTimeOutDeadlockEnabled() &&
         (clock()>timeOutShutdown)
      ) {
         tarch::parallel::Node::getInstance().triggerDeadlockTimeOut(
           "peano::heap::SynchronousDataExchanger",
           "receiveData()", fromRank,
           _metaDataTag, -1
         );
      }
      tarch::parallel::Node::getInstance().receiveDanglingMessages();

      resultTask = findMessageFromRankInReceiveBuffer(fromRank);

      if (SendAndReceiveHeapSynchronousDataBlocking>0) {
        usleep(SendAndReceiveHeapSynchronousDataBlocking);
      }
    }
  }

  result = extractMessageFromReceiveBuffer(resultTask, position, level);

  logTraceOutWith1Argument( "receiveData(...)", result.size() );
  return result;
}


template<class Data, bool CreateCopiesOfSentData>
void peano::heap::SynchronousDataExchanger<Data, CreateCopiesOfSentData>::plotStatistics() const {
  logInfo(
    "plotStatistics()",
    "records sent by " << _identifier << ": " <<
    _numberOfSentRecords
  );

  logInfo(
    "plotStatistics()",
    "messages sent by " << _identifier << ": " <<
    _numberOfSentMessages
  );

  logInfo(
    "plotStatistics()",
    "records received by " << _identifier << ": " <<
    _numberOfReceivedRecords
  );

  logInfo(
    "plotStatistics()",
    "messages received by " << _identifier << ": " <<
    _numberOfReceivedMessages
  );
}


template<class Data, bool CreateCopiesOfSentData>
void peano::heap::SynchronousDataExchanger<Data, CreateCopiesOfSentData>::clearStatistics() {
  _numberOfSentMessages     = 0;
  _numberOfSentRecords      = 0;
  _numberOfReceivedMessages = 0;
  _numberOfReceivedRecords  = 0;
}


template <class Data, bool CreateCopiesOfSentData>
typename std::list< peano::heap::SendReceiveTask<Data> >::iterator peano::heap::SynchronousDataExchanger<Data, CreateCopiesOfSentData>::findMessageFromRankInReceiveBuffer( int ofRank ) {
  for (typename std::list<SendReceiveTask<Data> >::iterator i = _receiveTasks.begin(); i != _receiveTasks.end(); i++) {
    if (i->_rank==ofRank) {
      MPI_Status status;
      int        finishedWait = false;
      if (i->_metaInformation.getLength() > 0) {
        const int mpiTestResult = MPI_Test(&(i->_request), &finishedWait, &status);
        if (mpiTestResult!=MPI_SUCCESS) {
          logError(
            "findMessageFromRankInReceiveBuffer()",
             "testing for messages failed: " << tarch::parallel::MPIReturnValueToString(mpiTestResult)
          );
        }
      }
      else {
        finishedWait = true;
      }

      if (finishedWait) {
        return i;
      }
      else {
        return _receiveTasks.end();
      }
    }
  }
  return _receiveTasks.end();
}


template <class Data, bool CreateCopiesOfSentData>
std::vector< Data > peano::heap::SynchronousDataExchanger<Data, CreateCopiesOfSentData>::extractMessageFromReceiveBuffer(
  typename std::list< SendReceiveTask<Data> >::iterator messageTask,
  const tarch::la::Vector<DIMENSIONS, double>&          position,
  int                                                   level
) {
  assertion( messageTask != _receiveTasks.end() );

  #ifdef Asserts
  const int numberOfElementsOfThisEntry = messageTask->_metaInformation.getLength();
  assertion2(numberOfElementsOfThisEntry >= 0, position, level);
  #endif

  assertion4(
    (messageTask->_metaInformation.getLevel() == level) || numberOfElementsOfThisEntry == 0,
    messageTask->_metaInformation.toString(),
    level,  position,
    tarch::parallel::Node::getInstance().getRank()
  );
  for (int d=0; d<DIMENSIONS; d++) {
    assertion5(
      tarch::la::equals(messageTask->_metaInformation.getPosition(d), position(d)),
      messageTask->_metaInformation.toString(),
      level,  position,  d,
      tarch::parallel::Node::getInstance().getRank()
    );
  }

  assertion(messageTask->_data!=0 || numberOfElementsOfThisEntry==0);

  const std::vector< Data > result = messageTask->unwrapDataAndFreeMemory();

  _receiveTasks.erase(messageTask);

  return result;
}
