#ifndef _PEANO_PARALLEL_MESSAGES_FORKMESSAGE_H
#define _PEANO_PARALLEL_MESSAGES_FORKMESSAGE_H

#include "peano/utils/Globals.h"
#include "tarch/compiler/CompilerSpecificSettings.h"
#include "peano/utils/PeanoOptimisations.h"
#ifdef Parallel
	#include "tarch/parallel/Node.h"
#endif
#ifdef Parallel
	#include <mpi.h>
#endif
#include "tarch/logging/Log.h"
#include "tarch/la/Vector.h"
#include <bitset>
#include <complex>
#include <string>
#include <iostream>

namespace peano {
   namespace parallel {
      namespace messages {
         class ForkMessage;
         class ForkMessagePacked;
      }
   }
}

/**
 * @author This class is generated by DaStGen
 * 		   DataStructureGenerator (DaStGen)
 * 		   2007-2009 Wolfgang Eckhardt
 * 		   2012      Tobias Weinzierl
 *
 * 		   build date: 09-02-2014 14:40
 *
 * @date   30/10/2014 20:39
 */
class peano::parallel::messages::ForkMessage { 
   
   public:
      
      typedef peano::parallel::messages::ForkMessagePacked Packed;
      
      struct PersistentRecords {
         tarch::la::Vector<DIMENSIONS,double> _domainOffset;
         tarch::la::Vector<DIMENSIONS,double> _h;
         tarch::la::Vector<TWO_POWER_D_TIMES_TWO_POWER_D,int> _adjacencyData;
         int _level;
         tarch::la::Vector<DIMENSIONS,int> _positionOfFineGridCellRelativeToCoarseGridCell;
         /**
          * Generated
          */
         PersistentRecords();
         
         /**
          * Generated
          */
         PersistentRecords(const tarch::la::Vector<DIMENSIONS,double>& domainOffset, const tarch::la::Vector<DIMENSIONS,double>& h, const tarch::la::Vector<TWO_POWER_D_TIMES_TWO_POWER_D,int>& adjacencyData, const int& level, const tarch::la::Vector<DIMENSIONS,int>& positionOfFineGridCellRelativeToCoarseGridCell);
         
          tarch::la::Vector<DIMENSIONS,double> getDomainOffset() const ;
         
          void setDomainOffset(const tarch::la::Vector<DIMENSIONS,double>& domainOffset) ;
         
          tarch::la::Vector<DIMENSIONS,double> getH() const ;
         
          void setH(const tarch::la::Vector<DIMENSIONS,double>& h) ;
         
          tarch::la::Vector<TWO_POWER_D_TIMES_TWO_POWER_D,int> getAdjacencyData() const ;
         
          void setAdjacencyData(const tarch::la::Vector<TWO_POWER_D_TIMES_TWO_POWER_D,int>& adjacencyData) ;
         
         /**
          * Generated
          */
          int getLevel() const ;
         
         /**
          * Generated
          */
          void setLevel(const int& level) ;
         
          tarch::la::Vector<DIMENSIONS,int> getPositionOfFineGridCellRelativeToCoarseGridCell() const ;
         
          void setPositionOfFineGridCellRelativeToCoarseGridCell(const tarch::la::Vector<DIMENSIONS,int>& positionOfFineGridCellRelativeToCoarseGridCell) ;
         
         
      };
      
   private: 
      PersistentRecords _persistentRecords;
      
   public:
      /**
       * Generated
       */
      ForkMessage();
      
      /**
       * Generated
       */
      ForkMessage(const PersistentRecords& persistentRecords);
      
      /**
       * Generated
       */
      ForkMessage(const tarch::la::Vector<DIMENSIONS,double>& domainOffset, const tarch::la::Vector<DIMENSIONS,double>& h, const tarch::la::Vector<TWO_POWER_D_TIMES_TWO_POWER_D,int>& adjacencyData, const int& level, const tarch::la::Vector<DIMENSIONS,int>& positionOfFineGridCellRelativeToCoarseGridCell);
      
      /**
       * Generated
       */
      virtual ~ForkMessage();
      
       tarch::la::Vector<DIMENSIONS,double> getDomainOffset() const ;
      
       void setDomainOffset(const tarch::la::Vector<DIMENSIONS,double>& domainOffset) ;
      
       double getDomainOffset(int elementIndex) const ;
      
       void setDomainOffset(int elementIndex, const double& domainOffset) ;
      
       tarch::la::Vector<DIMENSIONS,double> getH() const ;
      
       void setH(const tarch::la::Vector<DIMENSIONS,double>& h) ;
      
       double getH(int elementIndex) const ;
      
       void setH(int elementIndex, const double& h) ;
      
       tarch::la::Vector<TWO_POWER_D_TIMES_TWO_POWER_D,int> getAdjacencyData() const ;
      
       void setAdjacencyData(const tarch::la::Vector<TWO_POWER_D_TIMES_TWO_POWER_D,int>& adjacencyData) ;
      
       int getAdjacencyData(int elementIndex) const ;
      
       void setAdjacencyData(int elementIndex, const int& adjacencyData) ;
      
      /**
       * Generated
       */
       int getLevel() const ;
      
      /**
       * Generated
       */
       void setLevel(const int& level) ;
      
       tarch::la::Vector<DIMENSIONS,int> getPositionOfFineGridCellRelativeToCoarseGridCell() const ;
      
       void setPositionOfFineGridCellRelativeToCoarseGridCell(const tarch::la::Vector<DIMENSIONS,int>& positionOfFineGridCellRelativeToCoarseGridCell) ;
      
       int getPositionOfFineGridCellRelativeToCoarseGridCell(int elementIndex) const ;
      
       void setPositionOfFineGridCellRelativeToCoarseGridCell(int elementIndex, const int& positionOfFineGridCellRelativeToCoarseGridCell) ;
      
      /**
       * Generated
       */
      std::string toString() const;
      
      /**
       * Generated
       */
      void toString(std::ostream& out) const;
      
      
      PersistentRecords getPersistentRecords() const;
      /**
       * Generated
       */
      ForkMessagePacked convert() const;
      
      
   #ifdef Parallel
      protected:
         static tarch::logging::Log _log;
         
         int _senderDestinationRank;
         
      public:
         
         /**
          * Global that represents the mpi datatype.
          * There are two variants: Datatype identifies only those attributes marked with
          * parallelise. FullDatatype instead identifies the whole record with all fields.
          */
         static MPI_Datatype Datatype;
         static MPI_Datatype FullDatatype;
         
         /**
          * Initializes the data type for the mpi operations. Has to be called
          * before the very first send or receive operation is called.
          */
         static void initDatatype();
         
         static void shutdownDatatype();
         
         /**
          * @param communicateSleep -1 Data exchange through blocking mpi
          * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
          * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
          */
         void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
         
         void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
         
         static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
         
         int getSenderRank() const;
         
   #endif
      
   };
   
   /**
    * @author This class is generated by DaStGen
    * 		   DataStructureGenerator (DaStGen)
    * 		   2007-2009 Wolfgang Eckhardt
    * 		   2012      Tobias Weinzierl
    *
    * 		   build date: 09-02-2014 14:40
    *
    * @date   30/10/2014 20:39
    */
   class peano::parallel::messages::ForkMessagePacked { 
      
      public:
         
         struct PersistentRecords {
            tarch::la::Vector<DIMENSIONS,double> _domainOffset;
            tarch::la::Vector<DIMENSIONS,double> _h;
            tarch::la::Vector<TWO_POWER_D_TIMES_TWO_POWER_D,int> _adjacencyData;
            int _level;
            tarch::la::Vector<DIMENSIONS,int> _positionOfFineGridCellRelativeToCoarseGridCell;
            /**
             * Generated
             */
            PersistentRecords();
            
            /**
             * Generated
             */
            PersistentRecords(const tarch::la::Vector<DIMENSIONS,double>& domainOffset, const tarch::la::Vector<DIMENSIONS,double>& h, const tarch::la::Vector<TWO_POWER_D_TIMES_TWO_POWER_D,int>& adjacencyData, const int& level, const tarch::la::Vector<DIMENSIONS,int>& positionOfFineGridCellRelativeToCoarseGridCell);
            
             tarch::la::Vector<DIMENSIONS,double> getDomainOffset() const ;
            
             void setDomainOffset(const tarch::la::Vector<DIMENSIONS,double>& domainOffset) ;
            
             tarch::la::Vector<DIMENSIONS,double> getH() const ;
            
             void setH(const tarch::la::Vector<DIMENSIONS,double>& h) ;
            
             tarch::la::Vector<TWO_POWER_D_TIMES_TWO_POWER_D,int> getAdjacencyData() const ;
            
             void setAdjacencyData(const tarch::la::Vector<TWO_POWER_D_TIMES_TWO_POWER_D,int>& adjacencyData) ;
            
            /**
             * Generated
             */
             int getLevel() const ;
            
            /**
             * Generated
             */
             void setLevel(const int& level) ;
            
             tarch::la::Vector<DIMENSIONS,int> getPositionOfFineGridCellRelativeToCoarseGridCell() const ;
            
             void setPositionOfFineGridCellRelativeToCoarseGridCell(const tarch::la::Vector<DIMENSIONS,int>& positionOfFineGridCellRelativeToCoarseGridCell) ;
            
            
         };
         
      private: 
         PersistentRecords _persistentRecords;
         
      public:
         /**
          * Generated
          */
         ForkMessagePacked();
         
         /**
          * Generated
          */
         ForkMessagePacked(const PersistentRecords& persistentRecords);
         
         /**
          * Generated
          */
         ForkMessagePacked(const tarch::la::Vector<DIMENSIONS,double>& domainOffset, const tarch::la::Vector<DIMENSIONS,double>& h, const tarch::la::Vector<TWO_POWER_D_TIMES_TWO_POWER_D,int>& adjacencyData, const int& level, const tarch::la::Vector<DIMENSIONS,int>& positionOfFineGridCellRelativeToCoarseGridCell);
         
         /**
          * Generated
          */
         virtual ~ForkMessagePacked();
         
          tarch::la::Vector<DIMENSIONS,double> getDomainOffset() const ;
         
          void setDomainOffset(const tarch::la::Vector<DIMENSIONS,double>& domainOffset) ;
         
          double getDomainOffset(int elementIndex) const ;
         
          void setDomainOffset(int elementIndex, const double& domainOffset) ;
         
          tarch::la::Vector<DIMENSIONS,double> getH() const ;
         
          void setH(const tarch::la::Vector<DIMENSIONS,double>& h) ;
         
          double getH(int elementIndex) const ;
         
          void setH(int elementIndex, const double& h) ;
         
          tarch::la::Vector<TWO_POWER_D_TIMES_TWO_POWER_D,int> getAdjacencyData() const ;
         
          void setAdjacencyData(const tarch::la::Vector<TWO_POWER_D_TIMES_TWO_POWER_D,int>& adjacencyData) ;
         
          int getAdjacencyData(int elementIndex) const ;
         
          void setAdjacencyData(int elementIndex, const int& adjacencyData) ;
         
         /**
          * Generated
          */
          int getLevel() const ;
         
         /**
          * Generated
          */
          void setLevel(const int& level) ;
         
          tarch::la::Vector<DIMENSIONS,int> getPositionOfFineGridCellRelativeToCoarseGridCell() const ;
         
          void setPositionOfFineGridCellRelativeToCoarseGridCell(const tarch::la::Vector<DIMENSIONS,int>& positionOfFineGridCellRelativeToCoarseGridCell) ;
         
          int getPositionOfFineGridCellRelativeToCoarseGridCell(int elementIndex) const ;
         
          void setPositionOfFineGridCellRelativeToCoarseGridCell(int elementIndex, const int& positionOfFineGridCellRelativeToCoarseGridCell) ;
         
         /**
          * Generated
          */
         std::string toString() const;
         
         /**
          * Generated
          */
         void toString(std::ostream& out) const;
         
         
         PersistentRecords getPersistentRecords() const;
         /**
          * Generated
          */
         ForkMessage convert() const;
         
         
      #ifdef Parallel
         protected:
            static tarch::logging::Log _log;
            
            int _senderDestinationRank;
            
         public:
            
            /**
             * Global that represents the mpi datatype.
             * There are two variants: Datatype identifies only those attributes marked with
             * parallelise. FullDatatype instead identifies the whole record with all fields.
             */
            static MPI_Datatype Datatype;
            static MPI_Datatype FullDatatype;
            
            /**
             * Initializes the data type for the mpi operations. Has to be called
             * before the very first send or receive operation is called.
             */
            static void initDatatype();
            
            static void shutdownDatatype();
            
            /**
             * @param communicateSleep -1 Data exchange through blocking mpi
             * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
             * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
             */
            void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
            
            void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
            
            static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
            
            int getSenderRank() const;
            
      #endif
         
      };
      
      #endif
      
