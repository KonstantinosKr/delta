\section{Shared memory parallelisation}
  \label{section:parallelisation:shared-memory}


\chapterDescription
  {
    30 minutes.
  }
  {
    A working simulation code and a compiler that supports either OpenMP or
    Intel's Threading Building Blocks (TBB) \footnote{At the moment, Peano's
    OpenMP support is slightly outdated. All examples should work straightforwardly
    with TBB however}.
  }

In this section, we discuss how to parallelise a simulation code on a shared
memory architecture.
Hereby, we focus on Peano's parallelisation features. 
In mature, big applications, they are typically supplemented by further
parallelisation that is application-specific: Peano can run lots of routines in
parallel if it is correctly used. 
This way, we are able to exploit several cores.
To exploit modern multicore and manycore architectures, codes however also have
to use parallelised routines, linear algebra, and so forth.
This is an additional level of parallelism that cannot be tackled here.

\subsection{Preparation}

Peano compiles in parallel out-of-the-box if you use the PDT to setup a project
blueprint. 
To facilitate a shared memory parallel build, you have to translate your code
with the compile flag \texttt{-DSharedTBB}. 
Alternative variants are \texttt{-DSharedOMP}, e.g.
Once you edit your makefile and add this compile flag, please also provide the
correct include and link paths to the makefile.
For modern Intel compilers, no changes should be required, as Intel's TBB come
along with the compiler suite.

By default, the auto-generated \texttt{main} configures the parallel environment. 
While OpenMP relies on the setup of a well-suited thread count via environment
variables, TBB requires/allows the user to select a thread count manually. 
If you want to configure the thread count this way, please add the corresponding
instructions to your \texttt{main}.
To make your code portable (and to preserve a serial version), I recommend to
embed all shared memory-specific routines into ifdefs.
Besides the aforementioned \texttt{SharedXXX} defines, Peano also provides a
flag \texttt{SharedMemoryParallelisation} that is set as soon as OpenMP or TBB
is selected.
To make use of it, you have to include \texttt{MulticoreDefinitions.h}.

\begin{code}
#ifdef SharedMemoryParallelisation
#include "tarch/multicore/Core.h"
#endif

#include "tarch/multicore/MulticoreDefinitions.h"

  // should be generated by the PDT
  int sharedMemorySetup = peano::initSharedMemoryEnvironment();
  ...


  // manual configuration of threads (optional)
  #ifdef SharedMemoryParallelisation
  const int         numberOfCores    = 16;
  tarch::multicore::Core::getInstance().configure(numberOfCores);
  #endif

\end{code}

\noindent
Peano's kernel uses multiple threads in several places. 
However, most of these concurrent fragments are really very short-running.
As a result, it is not clear whether it pays off to use multithreading or not.
Therefore Peano uses an oracle---an object that returns per grid
traversal phase whether multitasking should be used or not.
This oracle can be found in \texttt{peano/datatraversal/autotuning}. 
Details on this oracle are discussed later.
For the time being, insert a commands into your runner.

\begin{code}
int mynamespace::runners::Runner::run() {
  #ifdef SharedMemoryParallelisation
  // We assume that the workload per cell is that big that we can set the enterCell
  // grain size to 1 as well as the minimum grain size. All the other values remain
  // the default values.
  peano::datatraversal::autotuning::Oracle::getInstance().setOracle(
    new peano::datatraversal::autotuning::OracleForOnePhaseDummy(true)
  );
  #endif
  ...
}
\end{code}

\noindent
The \texttt{true} parameters enables multicore support.
It might be reasonable to study the other parameters (see either the header
file or Peano's webpages) later.
Right now, it should be possible to recompile the code and to run a first
version on a shared memory machine.
It probably won't yield that much parallel speedup though \ldots


\subsection{Specifying concurrency levels}

Peano's fundamental idea is that users use events to say what is to be done. 
But codes leave it to the kernel to decide when and---anticipating some
constraints---in which order it is done.
This property is exploited by the multicore variant: 
Peano mappings specify whether multiple calls to one event (such as
\texttt{enterCell}) may run in parallel.
The kernel then decides autonomously whether to run in parallel and on which
cores.


To make this work, we have to revise the concept of an adapter. 
An adapter invokes multiple events.
Therefore, the concurrency level of an event has to be the most pessimistic
combination of the concurrency levels of all mappings realising this event.
This combination is automatically determined by the adapters generated by the
PDT.


Concurrency levels are specified within the events. 
Per event there is one concurrency specification. 
\texttt{touchVertexFirstTime}'s concurrency level for example is specified by
the routine \texttt{touchVertexFirstTimeSpecification}.
If you want to run \texttt{touchVertexFirstTime} in parallel, open all mappings
and edit \texttt{touchVertexFirstTimeSpecification} in each individual one.


A specification returns an instance of \texttt{peano::MappingSpecification}. 
Such an instance accepts parameters:
\begin{itemize}
  \item The first flag specifies whether the corresponding event works on the
  whole tree, only on its leaves, or whether it actually does not implement
  anything at all.
  \item The second flag specifies whether a particular event may run in
  parallel.
  \item Further flags specify whether events support resiliency and other
  experimental features. For most Peano kernel variants, such further flags are
  not supported. We maintain these variants in experimental Peano kernels only.
\end{itemize}

\begin{remark}
Even if you run your code without any shared memory parallelisation, it makes
sense to tailor all specifications. If your code does work on the finest tree
levels only, e.g., you can obtain significant speedup if you change the
\texttt{WholeTree} flag into \texttt{OnlyLeaves}. The most significant (serial)
speedups are obtained if you mark all specs as \texttt{Nop} where the actual
mapping does not do anything in the corresponding events. In this case, the
Peano kernel can skip whole function calls completely.
\end{remark}

\begin{remark}
If you tune/parallelise particular events and if you, at the same time, use
predefined mappings, you may have to study the specification objects
constructed there. Adapters always have to work pessimistically. If a predefined
mapping requires a particular event to be called sequentially (for plotters,
e.g.), you can specify any concurrency level you want---the kernel always will
run the whole event serially.
\end{remark}


For a quick start, I recommend to pick one particular event that is not
empty and where you do know exactly that it can run in parallel with other
events.
Select the correct concurrency level then:
\begin{itemize}
  \item Serial specifies that this event may not run in parallel and
  \item All other variants allow the kernel to issue events in parallel.
  However, they anticipate certain data dependencies---you may for example
  decide that entering a cell may be issued in parallel as long as no two events
  can access two adjacent vertices at the same time. This way, you anticipate
  data races.
\end{itemize}



\subsection{Ensuring inter-thread data consistency}

Once a parallel event is identified, the Peano kernel may run it on multiple
threads in parallel.
For this, the whole mapping object is replicated. 
The replication triggers the mapping's copy constructor.
Once the parallel phase is processed---all cells have been entered in parallel,
e.g.---all mapping replica are destroyed again and merged into one mapping that
is held by the adapter.
The mappings provide routines to plug into this life cycle.

In the copy constructor, you have to copy all mapping attributes that you need
in your parallel routines.
Two scenarios appear most often: Globally read properties such as a state object
are copied to each thread instance of the mapping.
Globally written attributes such as a global residual are set to zero in the
copy constructor:

\begin{code}
#if defined(SharedMemoryParallelisation)
mynamespace::MyMapping::MyMapping(const Collision&  masterThread):
  _localState( masterThread._localState ) {
  // alternatively, we could call
  // _localState = masterThread._localState;
  
  // now we clear all reduced/accumulated data:
  _localState.clearAttributes();
}
\end{code}

\noindent
The counterpart of the copy constructor is the routine
\texttt{mergeWithWorkerThread} that is invoked every time a mapping has been
replicated to run in parallel on multiple cores and this parallel phase is about
to terminate.
Peano does not use the destructor of the mapping to merge data to obtain a finer
control of thread replica and to be able to reuse mapping instances.

Usually, the merger only reduces globally accumulated data in this routine. 
If you have followed the recommendation in Section
\ref{section:applications:heat-equation} to make mappings hold copies of the
\texttt{State} object and to offer a merge routine, the mapping's shared
memory code resembles

\begin{code}
void mynamespace::MyMapping::::mergeWithWorkerThread(
  const mynamespace::MyMapping& workerThread
) {
  logTraceIn( "mergeWithWorkerThread(Collision)" );

  _localState.merge( workerThread._localState );
  
  logTraceOut( "mergeWithWorkerThread(Collision)" );
}
#endif

\end{code}



\subsection{Tailoring the oracle}

The oracle is the central point of control to decide whether event should be
invoked in parallel for given problem sizes.
The mapping specifications decide whether events may run in parallel.
The oracle specifies whether concurrent events should run in parallel for a
given problem size.


Whenever the kernel runs into a particular set of events and decides that it
would like to invoke those events in parallel, it realises the following
workflow:
\begin{itemize}
  \item Ask the adapter whether its combination of events allows the kernel to
  run events in parallel. If the result is a yes:
  \item Tell the oracle which adapter currently is active.
  \item Pass the oracle the problem size and ask which grain size (minimal
  problem chunk size) might be used. 
  \item If the adapter returns 0, nothing is ran in parallel. 0 should be
  returned by the oracle if the problem overall is too small to benefit from any
  shared memory parallelisation.
  \item Otherwise, split the problem into sizes of size at least grain size,
  replicate the mapping as often as required, and start using multiple threads.
\end{itemize}

\noindent
As clarified, each adapter is associated to one oracle, i.e.~you are able to
use oracles specifying grain and minimal problem sizes on a per-adapter base.
Furthermore, oracles are not static.
They have a state and thus can for example `learn' which grain sizes are
reasonable\footnote{We've written a paper on this a few years ago.}.
For a quick start, some trial and error with
\texttt{peano::datatraversal::autotuning::OracleForOnePhaseDummy} are usually
sufficient.
Just modify some of the predefined variables and study how the actual CPU usage
and the time-to-solution change.
Once you have detailed knowledge about your application's behaviour, it might
be reasonable to replace the Dummy with a tailored implementation of
\texttt{peano::datatraversal::autotuning::OracleForOnePhase}.
Peano's toolbox collection also contains generic, autotuning oracle
implementations.


When you study performance, it might be particular interesting that all oracles
can fed with real-time data about their performance and provide statistics. To
obtain the statistics, call
\begin{code}
  peano::datatraversal::autotuning::Oracle::getInstance().plotStatistics();
\end{code}
\noindent
If you do not need real-time measurements, please construct the oracle (see
\texttt{new} call in the snippet before) accordingly and disable the clocking.
It is expensive.


\subsection{Working with Peano's tasks, semaphores, locks and loops}

All shared memory parallelisation standards today provide tasks, semaphores,
locks, and so forth.
Peano provides a wrapper around those which allows you to create one
implementation that then runs with OpenMP, TBBs and so forth.
To use them, I recommend to study all classes stored in
\texttt{tarch::multicore}.
Notably \texttt{BooleanSemaphore} and \texttt{Lock} are of interest.

Furthermore, the header \texttt{Loop.h} might be of interest. 
It provides very useful macros:
\begin{itemize}
  \item \texttt{pfor} is a macro resembling a simple for-loop. If you compile
  your code with any shared memory compile flag, it makes the loop run in
  parallel.
  \item \texttt{pdfor} is a \texttt{pfor} as well. However, it does not traverse
  a linear sequence but runs over a $d$-dimensional index set. Very useful
  within a cell, e.g., where you have to do something with all vertices, while
  all vertices can be processed in parallel.
\end{itemize}



\subsection*{Further reading}

\begin{itemize}
  \item Weinzierl, Tobias, Bader, Michael, Unterweger, Kristof and Wittmann,
  Roland (2014). {\em Block Fusion on Dynamically Adaptive Spacetree Grids for
  Shallow Water Waves}. Parallel Processing Letters 24(3): 1441006.
  \item Schreiber, Martin, Weinzierl, Tobias and Bungartz, Hans-Joachim (2013).
  {\em Cluster Optimization and Parallelization of Simulations with Dynamically
  Adaptive Grids}. Euro-Par 2013, Berlin Heidelberg, Springer-Verlag.
  \item Schreiber, Martin, Weinzierl, Tobias and Bungartz, Hans-Joachim (2013).
  {\em SFC-based Communication Metadata Encoding for Adaptive Mesh}. Proceedings
  of the International Conference on Parallel Computing (ParCo), IOS Press.
  \item Nogina, Svetlana, Unterweger, Kristof and Weinzierl, Tobias (2012).
  {\em Autotuning of Adaptive Mesh Refinement PDE Solvers on Shared Memory
  Architectures}. PPAM 2011, Heidelberg, Berlin, Springer-Verlag.
  \item Eckhardt, Wolfgang and Weinzierl, Tobias (2010). {\em A Blocking
  Strategy on Multicore Architectures for Dynamically Adaptive PDE Solvers}.
  Parallel Processing and Applied Mathematics, PPAM 2009, Springer-Verlag.
\end{itemize}
