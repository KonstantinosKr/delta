#ifndef _DEM_RECORDS_CELL_H
#define _DEM_RECORDS_CELL_H

#include "tarch/multicore/MulticoreDefinitions.h"
#include "peano/utils/Globals.h"
#include "tarch/compiler/CompilerSpecificSettings.h"
#include "peano/utils/PeanoOptimisations.h"
#ifdef Parallel
	#include "tarch/parallel/Node.h"
#endif
#ifdef Parallel
	#include <mpi.h>
#endif
#include "tarch/logging/Log.h"
#include "tarch/la/Vector.h"
#include <bitset>
#include <complex>
#include <string>
#include <iostream>

namespace dem {
   namespace records {
      class Cell;
      class CellPacked;
   }
}

#if defined(Parallel) && defined(Debug) && !defined(SharedMemoryParallelisation)
   /**
    * @author This class is generated by DaStGen
    * 		   DataStructureGenerator (DaStGen)
    * 		   2007-2009 Wolfgang Eckhardt
    * 		   2012      Tobias Weinzierl
    *
    * 		   build date: 09-02-2014 14:40
    *
    * @date   23/10/2016 00:08
    */
   class dem::records::Cell { 
      
      public:
         
         typedef dem::records::CellPacked Packed;
         
         enum State {
            Leaf = 0, Refined = 1, Root = 2
         };
         
         struct PersistentRecords {
            bool _isInside;
            State _state;
            int _level;
            #ifdef UseManualAlignment
            std::bitset<DIMENSIONS> _evenFlags __attribute__((aligned(VectorisationAlignment)));
            #else
            std::bitset<DIMENSIONS> _evenFlags;
            #endif
            #ifdef UseManualAlignment
            tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
            #endif
            int _responsibleRank;
            bool _subtreeHoldsWorker;
            bool _cellIsAForkCandidate;
            /**
             * Generated
             */
            PersistentRecords();
            
            /**
             * Generated
             */
            PersistentRecords(const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const bool& cellIsAForkCandidate);
            
            
            inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _isInside;
            }
            
            
            
            inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _isInside = isInside;
            }
            
            
            
            inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _state;
            }
            
            
            
            inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _state = state;
            }
            
            
            
            inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _level;
            }
            
            
            
            inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _level = level;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _evenFlags;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _evenFlags = (evenFlags);
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _accessNumber;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _accessNumber = (accessNumber);
            }
            
            
            
            inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _responsibleRank;
            }
            
            
            
            inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _responsibleRank = responsibleRank;
            }
            
            
            
            inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _subtreeHoldsWorker;
            }
            
            
            
            inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _subtreeHoldsWorker = subtreeHoldsWorker;
            }
            
            
            
            inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _cellIsAForkCandidate;
            }
            
            
            
            inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _cellIsAForkCandidate = cellIsAForkCandidate;
            }
            
            
            
         };
         
      private: 
         PersistentRecords _persistentRecords;
         
      public:
         /**
          * Generated
          */
         Cell();
         
         /**
          * Generated
          */
         Cell(const PersistentRecords& persistentRecords);
         
         /**
          * Generated
          */
         Cell(const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const bool& cellIsAForkCandidate);
         
         /**
          * Generated
          */
         virtual ~Cell();
         
         
         inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._isInside;
         }
         
         
         
         inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._isInside = isInside;
         }
         
         
         
         inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._state;
         }
         
         
         
         inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._state = state;
         }
         
         
         
         inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._level;
         }
         
         
         
         inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._level = level;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._evenFlags;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._evenFlags = (evenFlags);
         }
         
         
         
         inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            return _persistentRecords._evenFlags[elementIndex];
            
         }
         
         
         
         inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            _persistentRecords._evenFlags[elementIndex]= evenFlags;
            
         }
         
         
         
         inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            _persistentRecords._evenFlags.flip(elementIndex);
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._accessNumber;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._accessNumber = (accessNumber);
         }
         
         
         
         inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            return _persistentRecords._accessNumber[elementIndex];
            
         }
         
         
         
         inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            _persistentRecords._accessNumber[elementIndex]= accessNumber;
            
         }
         
         
         
         inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._responsibleRank;
         }
         
         
         
         inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._responsibleRank = responsibleRank;
         }
         
         
         
         inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._subtreeHoldsWorker;
         }
         
         
         
         inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._subtreeHoldsWorker = subtreeHoldsWorker;
         }
         
         
         
         inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._cellIsAForkCandidate;
         }
         
         
         
         inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._cellIsAForkCandidate = cellIsAForkCandidate;
         }
         
         
         /**
          * Generated
          */
         static std::string toString(const State& param);
         
         /**
          * Generated
          */
         static std::string getStateMapping();
         
         /**
          * Generated
          */
         std::string toString() const;
         
         /**
          * Generated
          */
         void toString(std::ostream& out) const;
         
         
         PersistentRecords getPersistentRecords() const;
         /**
          * Generated
          */
         CellPacked convert() const;
         
         
      #ifdef Parallel
         protected:
            static tarch::logging::Log _log;
            
            int _senderDestinationRank;
            
         public:
            
            /**
             * Global that represents the mpi datatype.
             * There are two variants: Datatype identifies only those attributes marked with
             * parallelise. FullDatatype instead identifies the whole record with all fields.
             */
            static MPI_Datatype Datatype;
            static MPI_Datatype FullDatatype;
            
            /**
             * Initializes the data type for the mpi operations. Has to be called
             * before the very first send or receive operation is called.
             */
            static void initDatatype();
            
            static void shutdownDatatype();
            
            /**
             * @param communicateSleep -1 Data exchange through blocking mpi
             * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
             * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
             */
            void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
            
            void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
            
            static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
            
            int getSenderRank() const;
            
      #endif
         
      };
      
      #ifndef DaStGenPackedPadding
        #define DaStGenPackedPadding 1      // 32 bit version
        // #define DaStGenPackedPadding 2   // 64 bit version
      #endif
      
      
      #ifdef PackedRecords
         #pragma pack (push, DaStGenPackedPadding)
      #endif
      
      /**
       * @author This class is generated by DaStGen
       * 		   DataStructureGenerator (DaStGen)
       * 		   2007-2009 Wolfgang Eckhardt
       * 		   2012      Tobias Weinzierl
       *
       * 		   build date: 09-02-2014 14:40
       *
       * @date   23/10/2016 00:08
       */
      class dem::records::CellPacked { 
         
         public:
            
            typedef dem::records::Cell::State State;
            
            struct PersistentRecords {
               int _level;
               tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
               int _responsibleRank;
               bool _subtreeHoldsWorker;
               
               /** mapping of records:
               || Member 	|| startbit 	|| length
                |  isInside	| startbit 0	| #bits 1
                |  state	| startbit 1	| #bits 2
                |  evenFlags	| startbit 3	| #bits DIMENSIONS
                |  cellIsAForkCandidate	| startbit DIMENSIONS + 3	| #bits 1
                */
               short int _packedRecords0;
               
               /**
                * Generated
                */
               PersistentRecords();
               
               /**
                * Generated
                */
               PersistentRecords(const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const bool& cellIsAForkCandidate);
               
               
               inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  short int mask = 1 << (0);
   _packedRecords0 = static_cast<short int>( isInside ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
               }
               
               
               
               inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | static_cast<short int>(state) << (1));
               }
               
               
               
               inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _level;
               }
               
               
               
               inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _level = level;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                  mask = static_cast<short int>(mask << (3));
                  short int tmp = static_cast<short int>(_packedRecords0 & mask);
                  tmp = static_cast<short int>(tmp >> (3));
                  std::bitset<DIMENSIONS> result = tmp;
                  return result;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                  mask = static_cast<short int>(mask << (3));
                  _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
                  _packedRecords0 = static_cast<short int>(_packedRecords0 | evenFlags.to_ulong() << (3));
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _accessNumber;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _accessNumber = (accessNumber);
               }
               
               
               
               inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _responsibleRank;
               }
               
               
               
               inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _responsibleRank = responsibleRank;
               }
               
               
               
               inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _subtreeHoldsWorker;
               }
               
               
               
               inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _subtreeHoldsWorker = subtreeHoldsWorker;
               }
               
               
               
               inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  short int mask = 1 << (DIMENSIONS + 3);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  short int mask = 1 << (DIMENSIONS + 3);
   _packedRecords0 = static_cast<short int>( cellIsAForkCandidate ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
            };
            
         private: 
            PersistentRecords _persistentRecords;
            
         public:
            /**
             * Generated
             */
            CellPacked();
            
            /**
             * Generated
             */
            CellPacked(const PersistentRecords& persistentRecords);
            
            /**
             * Generated
             */
            CellPacked(const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const bool& cellIsAForkCandidate);
            
            /**
             * Generated
             */
            virtual ~CellPacked();
            
            
            inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               short int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<short int>( isInside ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
            }
            
            
            
            inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
            }
            
            
            
            inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | static_cast<short int>(state) << (1));
            }
            
            
            
            inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._level;
            }
            
            
            
            inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._level = level;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
               mask = static_cast<short int>(mask << (3));
               short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
               tmp = static_cast<short int>(tmp >> (3));
               std::bitset<DIMENSIONS> result = tmp;
               return result;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
               mask = static_cast<short int>(mask << (3));
               _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
               _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | evenFlags.to_ulong() << (3));
            }
            
            
            
            inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               int mask = 1 << (3);
               mask = mask << elementIndex;
               return (_persistentRecords._packedRecords0& mask);
            }
            
            
            
            inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               assertion(!evenFlags || evenFlags==1);
               int shift        = 3 + elementIndex; 
               short int mask         = 1     << (shift);
               short int shiftedValue = evenFlags << (shift);
               _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 & ~mask;
               _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 |  shiftedValue;
            }
            
            
            
            inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               short int mask = 1 << (3);
               mask = mask << elementIndex;
               _persistentRecords._packedRecords0^= mask;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._accessNumber;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._accessNumber = (accessNumber);
            }
            
            
            
            inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               return _persistentRecords._accessNumber[elementIndex];
               
            }
            
            
            
            inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               _persistentRecords._accessNumber[elementIndex]= accessNumber;
               
            }
            
            
            
            inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._responsibleRank;
            }
            
            
            
            inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._responsibleRank = responsibleRank;
            }
            
            
            
            inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._subtreeHoldsWorker;
            }
            
            
            
            inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._subtreeHoldsWorker = subtreeHoldsWorker;
            }
            
            
            
            inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               short int mask = 1 << (DIMENSIONS + 3);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               short int mask = 1 << (DIMENSIONS + 3);
   _persistentRecords._packedRecords0 = static_cast<short int>( cellIsAForkCandidate ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
            }
            
            
            /**
             * Generated
             */
            static std::string toString(const State& param);
            
            /**
             * Generated
             */
            static std::string getStateMapping();
            
            /**
             * Generated
             */
            std::string toString() const;
            
            /**
             * Generated
             */
            void toString(std::ostream& out) const;
            
            
            PersistentRecords getPersistentRecords() const;
            /**
             * Generated
             */
            Cell convert() const;
            
            
         #ifdef Parallel
            protected:
               static tarch::logging::Log _log;
               
               int _senderDestinationRank;
               
            public:
               
               /**
                * Global that represents the mpi datatype.
                * There are two variants: Datatype identifies only those attributes marked with
                * parallelise. FullDatatype instead identifies the whole record with all fields.
                */
               static MPI_Datatype Datatype;
               static MPI_Datatype FullDatatype;
               
               /**
                * Initializes the data type for the mpi operations. Has to be called
                * before the very first send or receive operation is called.
                */
               static void initDatatype();
               
               static void shutdownDatatype();
               
               /**
                * @param communicateSleep -1 Data exchange through blocking mpi
                * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
                * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
                */
               void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
               
               void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
               
               static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
               
               int getSenderRank() const;
               
         #endif
            
         };
         
         #ifdef PackedRecords
         #pragma pack (pop)
         #endif
         
         
         
      #elif defined(Parallel) && !defined(Debug) && defined(SharedMemoryParallelisation)
         /**
          * @author This class is generated by DaStGen
          * 		   DataStructureGenerator (DaStGen)
          * 		   2007-2009 Wolfgang Eckhardt
          * 		   2012      Tobias Weinzierl
          *
          * 		   build date: 09-02-2014 14:40
          *
          * @date   23/10/2016 00:08
          */
         class dem::records::Cell { 
            
            public:
               
               typedef dem::records::CellPacked Packed;
               
               enum State {
                  Leaf = 0, Refined = 1, Root = 2
               };
               
               struct PersistentRecords {
                  bool _isInside;
                  State _state;
                  #ifdef UseManualAlignment
                  std::bitset<DIMENSIONS> _evenFlags __attribute__((aligned(VectorisationAlignment)));
                  #else
                  std::bitset<DIMENSIONS> _evenFlags;
                  #endif
                  #ifdef UseManualAlignment
                  tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber __attribute__((aligned(VectorisationAlignment)));
                  #else
                  tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
                  #endif
                  int _responsibleRank;
                  bool _subtreeHoldsWorker;
                  bool _cellIsAForkCandidate;
                  int _numberOfLoadsFromInputStream;
                  int _numberOfStoresToOutputStream;
                  /**
                   * Generated
                   */
                  PersistentRecords();
                  
                  /**
                   * Generated
                   */
                  PersistentRecords(const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const bool& cellIsAForkCandidate, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
                  
                  
                  inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _isInside;
                  }
                  
                  
                  
                  inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _isInside = isInside;
                  }
                  
                  
                  
                  inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _state;
                  }
                  
                  
                  
                  inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _state = state;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _evenFlags;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _evenFlags = (evenFlags);
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _accessNumber;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _accessNumber = (accessNumber);
                  }
                  
                  
                  
                  inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _responsibleRank;
                  }
                  
                  
                  
                  inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _responsibleRank = responsibleRank;
                  }
                  
                  
                  
                  inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _subtreeHoldsWorker;
                  }
                  
                  
                  
                  inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _subtreeHoldsWorker = subtreeHoldsWorker;
                  }
                  
                  
                  
                  inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _cellIsAForkCandidate;
                  }
                  
                  
                  
                  inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _cellIsAForkCandidate = cellIsAForkCandidate;
                  }
                  
                  
                  
                  inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _numberOfLoadsFromInputStream;
                  }
                  
                  
                  
                  inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
                  }
                  
                  
                  
                  inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _numberOfStoresToOutputStream;
                  }
                  
                  
                  
                  inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _numberOfStoresToOutputStream = numberOfStoresToOutputStream;
                  }
                  
                  
                  
               };
               
            private: 
               PersistentRecords _persistentRecords;
               
            public:
               /**
                * Generated
                */
               Cell();
               
               /**
                * Generated
                */
               Cell(const PersistentRecords& persistentRecords);
               
               /**
                * Generated
                */
               Cell(const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const bool& cellIsAForkCandidate, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
               
               /**
                * Generated
                */
               virtual ~Cell();
               
               
               inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._isInside;
               }
               
               
               
               inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._isInside = isInside;
               }
               
               
               
               inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._state;
               }
               
               
               
               inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._state = state;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._evenFlags;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._evenFlags = (evenFlags);
               }
               
               
               
               inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  return _persistentRecords._evenFlags[elementIndex];
                  
               }
               
               
               
               inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._evenFlags[elementIndex]= evenFlags;
                  
               }
               
               
               
               inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._evenFlags.flip(elementIndex);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._accessNumber;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._accessNumber = (accessNumber);
               }
               
               
               
               inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  return _persistentRecords._accessNumber[elementIndex];
                  
               }
               
               
               
               inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  _persistentRecords._accessNumber[elementIndex]= accessNumber;
                  
               }
               
               
               
               inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._responsibleRank;
               }
               
               
               
               inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._responsibleRank = responsibleRank;
               }
               
               
               
               inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._subtreeHoldsWorker;
               }
               
               
               
               inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._subtreeHoldsWorker = subtreeHoldsWorker;
               }
               
               
               
               inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._cellIsAForkCandidate;
               }
               
               
               
               inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._cellIsAForkCandidate = cellIsAForkCandidate;
               }
               
               
               
               inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._numberOfLoadsFromInputStream;
               }
               
               
               
               inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
               }
               
               
               
               inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._numberOfStoresToOutputStream;
               }
               
               
               
               inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._numberOfStoresToOutputStream = numberOfStoresToOutputStream;
               }
               
               
               /**
                * Generated
                */
               static std::string toString(const State& param);
               
               /**
                * Generated
                */
               static std::string getStateMapping();
               
               /**
                * Generated
                */
               std::string toString() const;
               
               /**
                * Generated
                */
               void toString(std::ostream& out) const;
               
               
               PersistentRecords getPersistentRecords() const;
               /**
                * Generated
                */
               CellPacked convert() const;
               
               
            #ifdef Parallel
               protected:
                  static tarch::logging::Log _log;
                  
                  int _senderDestinationRank;
                  
               public:
                  
                  /**
                   * Global that represents the mpi datatype.
                   * There are two variants: Datatype identifies only those attributes marked with
                   * parallelise. FullDatatype instead identifies the whole record with all fields.
                   */
                  static MPI_Datatype Datatype;
                  static MPI_Datatype FullDatatype;
                  
                  /**
                   * Initializes the data type for the mpi operations. Has to be called
                   * before the very first send or receive operation is called.
                   */
                  static void initDatatype();
                  
                  static void shutdownDatatype();
                  
                  /**
                   * @param communicateSleep -1 Data exchange through blocking mpi
                   * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
                   * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
                   */
                  void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                  
                  void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                  
                  static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  int getSenderRank() const;
                  
            #endif
               
            };
            
            #ifndef DaStGenPackedPadding
              #define DaStGenPackedPadding 1      // 32 bit version
              // #define DaStGenPackedPadding 2   // 64 bit version
            #endif
            
            
            #ifdef PackedRecords
               #pragma pack (push, DaStGenPackedPadding)
            #endif
            
            /**
             * @author This class is generated by DaStGen
             * 		   DataStructureGenerator (DaStGen)
             * 		   2007-2009 Wolfgang Eckhardt
             * 		   2012      Tobias Weinzierl
             *
             * 		   build date: 09-02-2014 14:40
             *
             * @date   23/10/2016 00:08
             */
            class dem::records::CellPacked { 
               
               public:
                  
                  typedef dem::records::Cell::State State;
                  
                  struct PersistentRecords {
                     tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
                     int _responsibleRank;
                     bool _subtreeHoldsWorker;
                     int _numberOfLoadsFromInputStream;
                     int _numberOfStoresToOutputStream;
                     
                     /** mapping of records:
                     || Member 	|| startbit 	|| length
                      |  isInside	| startbit 0	| #bits 1
                      |  state	| startbit 1	| #bits 2
                      |  evenFlags	| startbit 3	| #bits DIMENSIONS
                      |  cellIsAForkCandidate	| startbit DIMENSIONS + 3	| #bits 1
                      */
                     short int _packedRecords0;
                     
                     /**
                      * Generated
                      */
                     PersistentRecords();
                     
                     /**
                      * Generated
                      */
                     PersistentRecords(const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const bool& cellIsAForkCandidate, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
                     
                     
                     inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                     }
                     
                     
                     
                     inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = 1 << (0);
   _packedRecords0 = static_cast<short int>( isInside ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                     }
                     
                     
                     
                     inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
                     }
                     
                     
                     
                     inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | static_cast<short int>(state) << (1));
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                        mask = static_cast<short int>(mask << (3));
                        short int tmp = static_cast<short int>(_packedRecords0 & mask);
                        tmp = static_cast<short int>(tmp >> (3));
                        std::bitset<DIMENSIONS> result = tmp;
                        return result;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                        mask = static_cast<short int>(mask << (3));
                        _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
                        _packedRecords0 = static_cast<short int>(_packedRecords0 | evenFlags.to_ulong() << (3));
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _accessNumber;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _accessNumber = (accessNumber);
                     }
                     
                     
                     
                     inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _responsibleRank;
                     }
                     
                     
                     
                     inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _responsibleRank = responsibleRank;
                     }
                     
                     
                     
                     inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _subtreeHoldsWorker;
                     }
                     
                     
                     
                     inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _subtreeHoldsWorker = subtreeHoldsWorker;
                     }
                     
                     
                     
                     inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = 1 << (DIMENSIONS + 3);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                     }
                     
                     
                     
                     inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = 1 << (DIMENSIONS + 3);
   _packedRecords0 = static_cast<short int>( cellIsAForkCandidate ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                     }
                     
                     
                     
                     inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _numberOfLoadsFromInputStream;
                     }
                     
                     
                     
                     inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
                     }
                     
                     
                     
                     inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _numberOfStoresToOutputStream;
                     }
                     
                     
                     
                     inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _numberOfStoresToOutputStream = numberOfStoresToOutputStream;
                     }
                     
                     
                     
                  };
                  
               private: 
                  PersistentRecords _persistentRecords;
                  
               public:
                  /**
                   * Generated
                   */
                  CellPacked();
                  
                  /**
                   * Generated
                   */
                  CellPacked(const PersistentRecords& persistentRecords);
                  
                  /**
                   * Generated
                   */
                  CellPacked(const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const bool& cellIsAForkCandidate, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
                  
                  /**
                   * Generated
                   */
                  virtual ~CellPacked();
                  
                  
                  inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                  }
                  
                  
                  
                  inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     short int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<short int>( isInside ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                  }
                  
                  
                  
                  inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
                  }
                  
                  
                  
                  inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | static_cast<short int>(state) << (1));
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                     mask = static_cast<short int>(mask << (3));
                     short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
                     tmp = static_cast<short int>(tmp >> (3));
                     std::bitset<DIMENSIONS> result = tmp;
                     return result;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                     mask = static_cast<short int>(mask << (3));
                     _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
                     _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | evenFlags.to_ulong() << (3));
                  }
                  
                  
                  
                  inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     int mask = 1 << (3);
                     mask = mask << elementIndex;
                     return (_persistentRecords._packedRecords0& mask);
                  }
                  
                  
                  
                  inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     assertion(!evenFlags || evenFlags==1);
                     int shift        = 3 + elementIndex; 
                     short int mask         = 1     << (shift);
                     short int shiftedValue = evenFlags << (shift);
                     _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 & ~mask;
                     _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 |  shiftedValue;
                  }
                  
                  
                  
                  inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     short int mask = 1 << (3);
                     mask = mask << elementIndex;
                     _persistentRecords._packedRecords0^= mask;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._accessNumber;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._accessNumber = (accessNumber);
                  }
                  
                  
                  
                  inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                     return _persistentRecords._accessNumber[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                     _persistentRecords._accessNumber[elementIndex]= accessNumber;
                     
                  }
                  
                  
                  
                  inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._responsibleRank;
                  }
                  
                  
                  
                  inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._responsibleRank = responsibleRank;
                  }
                  
                  
                  
                  inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._subtreeHoldsWorker;
                  }
                  
                  
                  
                  inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._subtreeHoldsWorker = subtreeHoldsWorker;
                  }
                  
                  
                  
                  inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     short int mask = 1 << (DIMENSIONS + 3);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                  }
                  
                  
                  
                  inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     short int mask = 1 << (DIMENSIONS + 3);
   _persistentRecords._packedRecords0 = static_cast<short int>( cellIsAForkCandidate ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                  }
                  
                  
                  
                  inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._numberOfLoadsFromInputStream;
                  }
                  
                  
                  
                  inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
                  }
                  
                  
                  
                  inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._numberOfStoresToOutputStream;
                  }
                  
                  
                  
                  inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._numberOfStoresToOutputStream = numberOfStoresToOutputStream;
                  }
                  
                  
                  /**
                   * Generated
                   */
                  static std::string toString(const State& param);
                  
                  /**
                   * Generated
                   */
                  static std::string getStateMapping();
                  
                  /**
                   * Generated
                   */
                  std::string toString() const;
                  
                  /**
                   * Generated
                   */
                  void toString(std::ostream& out) const;
                  
                  
                  PersistentRecords getPersistentRecords() const;
                  /**
                   * Generated
                   */
                  Cell convert() const;
                  
                  
               #ifdef Parallel
                  protected:
                     static tarch::logging::Log _log;
                     
                     int _senderDestinationRank;
                     
                  public:
                     
                     /**
                      * Global that represents the mpi datatype.
                      * There are two variants: Datatype identifies only those attributes marked with
                      * parallelise. FullDatatype instead identifies the whole record with all fields.
                      */
                     static MPI_Datatype Datatype;
                     static MPI_Datatype FullDatatype;
                     
                     /**
                      * Initializes the data type for the mpi operations. Has to be called
                      * before the very first send or receive operation is called.
                      */
                     static void initDatatype();
                     
                     static void shutdownDatatype();
                     
                     /**
                      * @param communicateSleep -1 Data exchange through blocking mpi
                      * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
                      * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
                      */
                     void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                     
                     void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                     
                     static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                     
                     int getSenderRank() const;
                     
               #endif
                  
               };
               
               #ifdef PackedRecords
               #pragma pack (pop)
               #endif
               
               
               
            
         #elif defined(Debug) && !defined(Parallel) && defined(SharedMemoryParallelisation)
            /**
             * @author This class is generated by DaStGen
             * 		   DataStructureGenerator (DaStGen)
             * 		   2007-2009 Wolfgang Eckhardt
             * 		   2012      Tobias Weinzierl
             *
             * 		   build date: 09-02-2014 14:40
             *
             * @date   23/10/2016 00:08
             */
            class dem::records::Cell { 
               
               public:
                  
                  typedef dem::records::CellPacked Packed;
                  
                  enum State {
                     Leaf = 0, Refined = 1, Root = 2
                  };
                  
                  struct PersistentRecords {
                     bool _isInside;
                     State _state;
                     int _level;
                     #ifdef UseManualAlignment
                     std::bitset<DIMENSIONS> _evenFlags __attribute__((aligned(VectorisationAlignment)));
                     #else
                     std::bitset<DIMENSIONS> _evenFlags;
                     #endif
                     #ifdef UseManualAlignment
                     tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber __attribute__((aligned(VectorisationAlignment)));
                     #else
                     tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
                     #endif
                     int _numberOfLoadsFromInputStream;
                     int _numberOfStoresToOutputStream;
                     /**
                      * Generated
                      */
                     PersistentRecords();
                     
                     /**
                      * Generated
                      */
                     PersistentRecords(const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
                     
                     
                     inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _isInside;
                     }
                     
                     
                     
                     inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _isInside = isInside;
                     }
                     
                     
                     
                     inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _state;
                     }
                     
                     
                     
                     inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _state = state;
                     }
                     
                     
                     
                     inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _level;
                     }
                     
                     
                     
                     inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _level = level;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _evenFlags;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _evenFlags = (evenFlags);
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _accessNumber;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _accessNumber = (accessNumber);
                     }
                     
                     
                     
                     inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _numberOfLoadsFromInputStream;
                     }
                     
                     
                     
                     inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
                     }
                     
                     
                     
                     inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _numberOfStoresToOutputStream;
                     }
                     
                     
                     
                     inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _numberOfStoresToOutputStream = numberOfStoresToOutputStream;
                     }
                     
                     
                     
                  };
                  
               private: 
                  PersistentRecords _persistentRecords;
                  
               public:
                  /**
                   * Generated
                   */
                  Cell();
                  
                  /**
                   * Generated
                   */
                  Cell(const PersistentRecords& persistentRecords);
                  
                  /**
                   * Generated
                   */
                  Cell(const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
                  
                  /**
                   * Generated
                   */
                  virtual ~Cell();
                  
                  
                  inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._isInside;
                  }
                  
                  
                  
                  inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._isInside = isInside;
                  }
                  
                  
                  
                  inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._state;
                  }
                  
                  
                  
                  inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._state = state;
                  }
                  
                  
                  
                  inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._level;
                  }
                  
                  
                  
                  inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._level = level;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._evenFlags;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._evenFlags = (evenFlags);
                  }
                  
                  
                  
                  inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     return _persistentRecords._evenFlags[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     _persistentRecords._evenFlags[elementIndex]= evenFlags;
                     
                  }
                  
                  
                  
                  inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     _persistentRecords._evenFlags.flip(elementIndex);
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._accessNumber;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._accessNumber = (accessNumber);
                  }
                  
                  
                  
                  inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                     return _persistentRecords._accessNumber[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                     _persistentRecords._accessNumber[elementIndex]= accessNumber;
                     
                  }
                  
                  
                  
                  inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._numberOfLoadsFromInputStream;
                  }
                  
                  
                  
                  inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
                  }
                  
                  
                  
                  inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._numberOfStoresToOutputStream;
                  }
                  
                  
                  
                  inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._numberOfStoresToOutputStream = numberOfStoresToOutputStream;
                  }
                  
                  
                  /**
                   * Generated
                   */
                  static std::string toString(const State& param);
                  
                  /**
                   * Generated
                   */
                  static std::string getStateMapping();
                  
                  /**
                   * Generated
                   */
                  std::string toString() const;
                  
                  /**
                   * Generated
                   */
                  void toString(std::ostream& out) const;
                  
                  
                  PersistentRecords getPersistentRecords() const;
                  /**
                   * Generated
                   */
                  CellPacked convert() const;
                  
                  
               #ifdef Parallel
                  protected:
                     static tarch::logging::Log _log;
                     
                     int _senderDestinationRank;
                     
                  public:
                     
                     /**
                      * Global that represents the mpi datatype.
                      * There are two variants: Datatype identifies only those attributes marked with
                      * parallelise. FullDatatype instead identifies the whole record with all fields.
                      */
                     static MPI_Datatype Datatype;
                     static MPI_Datatype FullDatatype;
                     
                     /**
                      * Initializes the data type for the mpi operations. Has to be called
                      * before the very first send or receive operation is called.
                      */
                     static void initDatatype();
                     
                     static void shutdownDatatype();
                     
                     /**
                      * @param communicateSleep -1 Data exchange through blocking mpi
                      * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
                      * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
                      */
                     void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                     
                     void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                     
                     static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                     
                     int getSenderRank() const;
                     
               #endif
                  
               };
               
               #ifndef DaStGenPackedPadding
                 #define DaStGenPackedPadding 1      // 32 bit version
                 // #define DaStGenPackedPadding 2   // 64 bit version
               #endif
               
               
               #ifdef PackedRecords
                  #pragma pack (push, DaStGenPackedPadding)
               #endif
               
               /**
                * @author This class is generated by DaStGen
                * 		   DataStructureGenerator (DaStGen)
                * 		   2007-2009 Wolfgang Eckhardt
                * 		   2012      Tobias Weinzierl
                *
                * 		   build date: 09-02-2014 14:40
                *
                * @date   23/10/2016 00:08
                */
               class dem::records::CellPacked { 
                  
                  public:
                     
                     typedef dem::records::Cell::State State;
                     
                     struct PersistentRecords {
                        int _level;
                        tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
                        int _numberOfLoadsFromInputStream;
                        int _numberOfStoresToOutputStream;
                        
                        /** mapping of records:
                        || Member 	|| startbit 	|| length
                         |  isInside	| startbit 0	| #bits 1
                         |  state	| startbit 1	| #bits 2
                         |  evenFlags	| startbit 3	| #bits DIMENSIONS
                         */
                        short int _packedRecords0;
                        
                        /**
                         * Generated
                         */
                        PersistentRecords();
                        
                        /**
                         * Generated
                         */
                        PersistentRecords(const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
                        
                        
                        inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                        }
                        
                        
                        
                        inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = 1 << (0);
   _packedRecords0 = static_cast<short int>( isInside ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                        }
                        
                        
                        
                        inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
                        }
                        
                        
                        
                        inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | static_cast<short int>(state) << (1));
                        }
                        
                        
                        
                        inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _level;
                        }
                        
                        
                        
                        inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _level = level;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                           mask = static_cast<short int>(mask << (3));
                           short int tmp = static_cast<short int>(_packedRecords0 & mask);
                           tmp = static_cast<short int>(tmp >> (3));
                           std::bitset<DIMENSIONS> result = tmp;
                           return result;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                           mask = static_cast<short int>(mask << (3));
                           _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
                           _packedRecords0 = static_cast<short int>(_packedRecords0 | evenFlags.to_ulong() << (3));
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _accessNumber;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _accessNumber = (accessNumber);
                        }
                        
                        
                        
                        inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _numberOfLoadsFromInputStream;
                        }
                        
                        
                        
                        inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
                        }
                        
                        
                        
                        inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _numberOfStoresToOutputStream;
                        }
                        
                        
                        
                        inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _numberOfStoresToOutputStream = numberOfStoresToOutputStream;
                        }
                        
                        
                        
                     };
                     
                  private: 
                     PersistentRecords _persistentRecords;
                     
                  public:
                     /**
                      * Generated
                      */
                     CellPacked();
                     
                     /**
                      * Generated
                      */
                     CellPacked(const PersistentRecords& persistentRecords);
                     
                     /**
                      * Generated
                      */
                     CellPacked(const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
                     
                     /**
                      * Generated
                      */
                     virtual ~CellPacked();
                     
                     
                     inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                     }
                     
                     
                     
                     inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<short int>( isInside ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                     }
                     
                     
                     
                     inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
                     }
                     
                     
                     
                     inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | static_cast<short int>(state) << (1));
                     }
                     
                     
                     
                     inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._level;
                     }
                     
                     
                     
                     inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._level = level;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                        mask = static_cast<short int>(mask << (3));
                        short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
                        tmp = static_cast<short int>(tmp >> (3));
                        std::bitset<DIMENSIONS> result = tmp;
                        return result;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                        mask = static_cast<short int>(mask << (3));
                        _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
                        _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | evenFlags.to_ulong() << (3));
                     }
                     
                     
                     
                     inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<DIMENSIONS);
                        int mask = 1 << (3);
                        mask = mask << elementIndex;
                        return (_persistentRecords._packedRecords0& mask);
                     }
                     
                     
                     
                     inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<DIMENSIONS);
                        assertion(!evenFlags || evenFlags==1);
                        int shift        = 3 + elementIndex; 
                        short int mask         = 1     << (shift);
                        short int shiftedValue = evenFlags << (shift);
                        _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 & ~mask;
                        _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 |  shiftedValue;
                     }
                     
                     
                     
                     inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<DIMENSIONS);
                        short int mask = 1 << (3);
                        mask = mask << elementIndex;
                        _persistentRecords._packedRecords0^= mask;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._accessNumber;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._accessNumber = (accessNumber);
                     }
                     
                     
                     
                     inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                        return _persistentRecords._accessNumber[elementIndex];
                        
                     }
                     
                     
                     
                     inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                        _persistentRecords._accessNumber[elementIndex]= accessNumber;
                        
                     }
                     
                     
                     
                     inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._numberOfLoadsFromInputStream;
                     }
                     
                     
                     
                     inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
                     }
                     
                     
                     
                     inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._numberOfStoresToOutputStream;
                     }
                     
                     
                     
                     inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._numberOfStoresToOutputStream = numberOfStoresToOutputStream;
                     }
                     
                     
                     /**
                      * Generated
                      */
                     static std::string toString(const State& param);
                     
                     /**
                      * Generated
                      */
                     static std::string getStateMapping();
                     
                     /**
                      * Generated
                      */
                     std::string toString() const;
                     
                     /**
                      * Generated
                      */
                     void toString(std::ostream& out) const;
                     
                     
                     PersistentRecords getPersistentRecords() const;
                     /**
                      * Generated
                      */
                     Cell convert() const;
                     
                     
                  #ifdef Parallel
                     protected:
                        static tarch::logging::Log _log;
                        
                        int _senderDestinationRank;
                        
                     public:
                        
                        /**
                         * Global that represents the mpi datatype.
                         * There are two variants: Datatype identifies only those attributes marked with
                         * parallelise. FullDatatype instead identifies the whole record with all fields.
                         */
                        static MPI_Datatype Datatype;
                        static MPI_Datatype FullDatatype;
                        
                        /**
                         * Initializes the data type for the mpi operations. Has to be called
                         * before the very first send or receive operation is called.
                         */
                        static void initDatatype();
                        
                        static void shutdownDatatype();
                        
                        /**
                         * @param communicateSleep -1 Data exchange through blocking mpi
                         * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
                         * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
                         */
                        void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                        
                        void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                        
                        static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                        
                        int getSenderRank() const;
                        
                  #endif
                     
                  };
                  
                  #ifdef PackedRecords
                  #pragma pack (pop)
                  #endif
                  
                  
                  
               
            #elif defined(Parallel) && defined(Debug) && defined(SharedMemoryParallelisation)
               /**
                * @author This class is generated by DaStGen
                * 		   DataStructureGenerator (DaStGen)
                * 		   2007-2009 Wolfgang Eckhardt
                * 		   2012      Tobias Weinzierl
                *
                * 		   build date: 09-02-2014 14:40
                *
                * @date   23/10/2016 00:08
                */
               class dem::records::Cell { 
                  
                  public:
                     
                     typedef dem::records::CellPacked Packed;
                     
                     enum State {
                        Leaf = 0, Refined = 1, Root = 2
                     };
                     
                     struct PersistentRecords {
                        bool _isInside;
                        State _state;
                        int _level;
                        #ifdef UseManualAlignment
                        std::bitset<DIMENSIONS> _evenFlags __attribute__((aligned(VectorisationAlignment)));
                        #else
                        std::bitset<DIMENSIONS> _evenFlags;
                        #endif
                        #ifdef UseManualAlignment
                        tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber __attribute__((aligned(VectorisationAlignment)));
                        #else
                        tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
                        #endif
                        int _responsibleRank;
                        bool _subtreeHoldsWorker;
                        bool _cellIsAForkCandidate;
                        int _numberOfLoadsFromInputStream;
                        int _numberOfStoresToOutputStream;
                        /**
                         * Generated
                         */
                        PersistentRecords();
                        
                        /**
                         * Generated
                         */
                        PersistentRecords(const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const bool& cellIsAForkCandidate, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
                        
                        
                        inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _isInside;
                        }
                        
                        
                        
                        inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _isInside = isInside;
                        }
                        
                        
                        
                        inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _state;
                        }
                        
                        
                        
                        inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _state = state;
                        }
                        
                        
                        
                        inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _level;
                        }
                        
                        
                        
                        inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _level = level;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _evenFlags;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _evenFlags = (evenFlags);
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _accessNumber;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _accessNumber = (accessNumber);
                        }
                        
                        
                        
                        inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _responsibleRank;
                        }
                        
                        
                        
                        inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _responsibleRank = responsibleRank;
                        }
                        
                        
                        
                        inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _subtreeHoldsWorker;
                        }
                        
                        
                        
                        inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _subtreeHoldsWorker = subtreeHoldsWorker;
                        }
                        
                        
                        
                        inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _cellIsAForkCandidate;
                        }
                        
                        
                        
                        inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _cellIsAForkCandidate = cellIsAForkCandidate;
                        }
                        
                        
                        
                        inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _numberOfLoadsFromInputStream;
                        }
                        
                        
                        
                        inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
                        }
                        
                        
                        
                        inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _numberOfStoresToOutputStream;
                        }
                        
                        
                        
                        inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _numberOfStoresToOutputStream = numberOfStoresToOutputStream;
                        }
                        
                        
                        
                     };
                     
                  private: 
                     PersistentRecords _persistentRecords;
                     
                  public:
                     /**
                      * Generated
                      */
                     Cell();
                     
                     /**
                      * Generated
                      */
                     Cell(const PersistentRecords& persistentRecords);
                     
                     /**
                      * Generated
                      */
                     Cell(const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const bool& cellIsAForkCandidate, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
                     
                     /**
                      * Generated
                      */
                     virtual ~Cell();
                     
                     
                     inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._isInside;
                     }
                     
                     
                     
                     inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._isInside = isInside;
                     }
                     
                     
                     
                     inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._state;
                     }
                     
                     
                     
                     inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._state = state;
                     }
                     
                     
                     
                     inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._level;
                     }
                     
                     
                     
                     inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._level = level;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._evenFlags;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._evenFlags = (evenFlags);
                     }
                     
                     
                     
                     inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<DIMENSIONS);
                        return _persistentRecords._evenFlags[elementIndex];
                        
                     }
                     
                     
                     
                     inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<DIMENSIONS);
                        _persistentRecords._evenFlags[elementIndex]= evenFlags;
                        
                     }
                     
                     
                     
                     inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<DIMENSIONS);
                        _persistentRecords._evenFlags.flip(elementIndex);
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._accessNumber;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._accessNumber = (accessNumber);
                     }
                     
                     
                     
                     inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                        return _persistentRecords._accessNumber[elementIndex];
                        
                     }
                     
                     
                     
                     inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                        _persistentRecords._accessNumber[elementIndex]= accessNumber;
                        
                     }
                     
                     
                     
                     inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._responsibleRank;
                     }
                     
                     
                     
                     inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._responsibleRank = responsibleRank;
                     }
                     
                     
                     
                     inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._subtreeHoldsWorker;
                     }
                     
                     
                     
                     inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._subtreeHoldsWorker = subtreeHoldsWorker;
                     }
                     
                     
                     
                     inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._cellIsAForkCandidate;
                     }
                     
                     
                     
                     inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._cellIsAForkCandidate = cellIsAForkCandidate;
                     }
                     
                     
                     
                     inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._numberOfLoadsFromInputStream;
                     }
                     
                     
                     
                     inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
                     }
                     
                     
                     
                     inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._numberOfStoresToOutputStream;
                     }
                     
                     
                     
                     inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._numberOfStoresToOutputStream = numberOfStoresToOutputStream;
                     }
                     
                     
                     /**
                      * Generated
                      */
                     static std::string toString(const State& param);
                     
                     /**
                      * Generated
                      */
                     static std::string getStateMapping();
                     
                     /**
                      * Generated
                      */
                     std::string toString() const;
                     
                     /**
                      * Generated
                      */
                     void toString(std::ostream& out) const;
                     
                     
                     PersistentRecords getPersistentRecords() const;
                     /**
                      * Generated
                      */
                     CellPacked convert() const;
                     
                     
                  #ifdef Parallel
                     protected:
                        static tarch::logging::Log _log;
                        
                        int _senderDestinationRank;
                        
                     public:
                        
                        /**
                         * Global that represents the mpi datatype.
                         * There are two variants: Datatype identifies only those attributes marked with
                         * parallelise. FullDatatype instead identifies the whole record with all fields.
                         */
                        static MPI_Datatype Datatype;
                        static MPI_Datatype FullDatatype;
                        
                        /**
                         * Initializes the data type for the mpi operations. Has to be called
                         * before the very first send or receive operation is called.
                         */
                        static void initDatatype();
                        
                        static void shutdownDatatype();
                        
                        /**
                         * @param communicateSleep -1 Data exchange through blocking mpi
                         * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
                         * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
                         */
                        void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                        
                        void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                        
                        static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                        
                        int getSenderRank() const;
                        
                  #endif
                     
                  };
                  
                  #ifndef DaStGenPackedPadding
                    #define DaStGenPackedPadding 1      // 32 bit version
                    // #define DaStGenPackedPadding 2   // 64 bit version
                  #endif
                  
                  
                  #ifdef PackedRecords
                     #pragma pack (push, DaStGenPackedPadding)
                  #endif
                  
                  /**
                   * @author This class is generated by DaStGen
                   * 		   DataStructureGenerator (DaStGen)
                   * 		   2007-2009 Wolfgang Eckhardt
                   * 		   2012      Tobias Weinzierl
                   *
                   * 		   build date: 09-02-2014 14:40
                   *
                   * @date   23/10/2016 00:08
                   */
                  class dem::records::CellPacked { 
                     
                     public:
                        
                        typedef dem::records::Cell::State State;
                        
                        struct PersistentRecords {
                           int _level;
                           tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
                           int _responsibleRank;
                           bool _subtreeHoldsWorker;
                           int _numberOfLoadsFromInputStream;
                           int _numberOfStoresToOutputStream;
                           
                           /** mapping of records:
                           || Member 	|| startbit 	|| length
                            |  isInside	| startbit 0	| #bits 1
                            |  state	| startbit 1	| #bits 2
                            |  evenFlags	| startbit 3	| #bits DIMENSIONS
                            |  cellIsAForkCandidate	| startbit DIMENSIONS + 3	| #bits 1
                            */
                           short int _packedRecords0;
                           
                           /**
                            * Generated
                            */
                           PersistentRecords();
                           
                           /**
                            * Generated
                            */
                           PersistentRecords(const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const bool& cellIsAForkCandidate, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
                           
                           
                           inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                           }
                           
                           
                           
                           inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = 1 << (0);
   _packedRecords0 = static_cast<short int>( isInside ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                           }
                           
                           
                           
                           inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
                           }
                           
                           
                           
                           inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | static_cast<short int>(state) << (1));
                           }
                           
                           
                           
                           inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _level;
                           }
                           
                           
                           
                           inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _level = level;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                              mask = static_cast<short int>(mask << (3));
                              short int tmp = static_cast<short int>(_packedRecords0 & mask);
                              tmp = static_cast<short int>(tmp >> (3));
                              std::bitset<DIMENSIONS> result = tmp;
                              return result;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                              mask = static_cast<short int>(mask << (3));
                              _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
                              _packedRecords0 = static_cast<short int>(_packedRecords0 | evenFlags.to_ulong() << (3));
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _accessNumber;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _accessNumber = (accessNumber);
                           }
                           
                           
                           
                           inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _responsibleRank;
                           }
                           
                           
                           
                           inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _responsibleRank = responsibleRank;
                           }
                           
                           
                           
                           inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _subtreeHoldsWorker;
                           }
                           
                           
                           
                           inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _subtreeHoldsWorker = subtreeHoldsWorker;
                           }
                           
                           
                           
                           inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = 1 << (DIMENSIONS + 3);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                           }
                           
                           
                           
                           inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = 1 << (DIMENSIONS + 3);
   _packedRecords0 = static_cast<short int>( cellIsAForkCandidate ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                           }
                           
                           
                           
                           inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _numberOfLoadsFromInputStream;
                           }
                           
                           
                           
                           inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
                           }
                           
                           
                           
                           inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _numberOfStoresToOutputStream;
                           }
                           
                           
                           
                           inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _numberOfStoresToOutputStream = numberOfStoresToOutputStream;
                           }
                           
                           
                           
                        };
                        
                     private: 
                        PersistentRecords _persistentRecords;
                        
                     public:
                        /**
                         * Generated
                         */
                        CellPacked();
                        
                        /**
                         * Generated
                         */
                        CellPacked(const PersistentRecords& persistentRecords);
                        
                        /**
                         * Generated
                         */
                        CellPacked(const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const bool& cellIsAForkCandidate, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
                        
                        /**
                         * Generated
                         */
                        virtual ~CellPacked();
                        
                        
                        inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                        }
                        
                        
                        
                        inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<short int>( isInside ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                        }
                        
                        
                        
                        inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
                        }
                        
                        
                        
                        inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | static_cast<short int>(state) << (1));
                        }
                        
                        
                        
                        inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._level;
                        }
                        
                        
                        
                        inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._level = level;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                           mask = static_cast<short int>(mask << (3));
                           short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
                           tmp = static_cast<short int>(tmp >> (3));
                           std::bitset<DIMENSIONS> result = tmp;
                           return result;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                           mask = static_cast<short int>(mask << (3));
                           _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
                           _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | evenFlags.to_ulong() << (3));
                        }
                        
                        
                        
                        inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<DIMENSIONS);
                           int mask = 1 << (3);
                           mask = mask << elementIndex;
                           return (_persistentRecords._packedRecords0& mask);
                        }
                        
                        
                        
                        inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<DIMENSIONS);
                           assertion(!evenFlags || evenFlags==1);
                           int shift        = 3 + elementIndex; 
                           short int mask         = 1     << (shift);
                           short int shiftedValue = evenFlags << (shift);
                           _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 & ~mask;
                           _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 |  shiftedValue;
                        }
                        
                        
                        
                        inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<DIMENSIONS);
                           short int mask = 1 << (3);
                           mask = mask << elementIndex;
                           _persistentRecords._packedRecords0^= mask;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._accessNumber;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._accessNumber = (accessNumber);
                        }
                        
                        
                        
                        inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                           return _persistentRecords._accessNumber[elementIndex];
                           
                        }
                        
                        
                        
                        inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                           _persistentRecords._accessNumber[elementIndex]= accessNumber;
                           
                        }
                        
                        
                        
                        inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._responsibleRank;
                        }
                        
                        
                        
                        inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._responsibleRank = responsibleRank;
                        }
                        
                        
                        
                        inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._subtreeHoldsWorker;
                        }
                        
                        
                        
                        inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._subtreeHoldsWorker = subtreeHoldsWorker;
                        }
                        
                        
                        
                        inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = 1 << (DIMENSIONS + 3);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                        }
                        
                        
                        
                        inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = 1 << (DIMENSIONS + 3);
   _persistentRecords._packedRecords0 = static_cast<short int>( cellIsAForkCandidate ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                        }
                        
                        
                        
                        inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._numberOfLoadsFromInputStream;
                        }
                        
                        
                        
                        inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
                        }
                        
                        
                        
                        inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._numberOfStoresToOutputStream;
                        }
                        
                        
                        
                        inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._numberOfStoresToOutputStream = numberOfStoresToOutputStream;
                        }
                        
                        
                        /**
                         * Generated
                         */
                        static std::string toString(const State& param);
                        
                        /**
                         * Generated
                         */
                        static std::string getStateMapping();
                        
                        /**
                         * Generated
                         */
                        std::string toString() const;
                        
                        /**
                         * Generated
                         */
                        void toString(std::ostream& out) const;
                        
                        
                        PersistentRecords getPersistentRecords() const;
                        /**
                         * Generated
                         */
                        Cell convert() const;
                        
                        
                     #ifdef Parallel
                        protected:
                           static tarch::logging::Log _log;
                           
                           int _senderDestinationRank;
                           
                        public:
                           
                           /**
                            * Global that represents the mpi datatype.
                            * There are two variants: Datatype identifies only those attributes marked with
                            * parallelise. FullDatatype instead identifies the whole record with all fields.
                            */
                           static MPI_Datatype Datatype;
                           static MPI_Datatype FullDatatype;
                           
                           /**
                            * Initializes the data type for the mpi operations. Has to be called
                            * before the very first send or receive operation is called.
                            */
                           static void initDatatype();
                           
                           static void shutdownDatatype();
                           
                           /**
                            * @param communicateSleep -1 Data exchange through blocking mpi
                            * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
                            * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
                            */
                           void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                           
                           void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                           
                           static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                           
                           int getSenderRank() const;
                           
                     #endif
                        
                     };
                     
                     #ifdef PackedRecords
                     #pragma pack (pop)
                     #endif
                     
                     
                     
                  
               #elif !defined(Parallel) && !defined(Debug) && !defined(SharedMemoryParallelisation)
                  /**
                   * @author This class is generated by DaStGen
                   * 		   DataStructureGenerator (DaStGen)
                   * 		   2007-2009 Wolfgang Eckhardt
                   * 		   2012      Tobias Weinzierl
                   *
                   * 		   build date: 09-02-2014 14:40
                   *
                   * @date   23/10/2016 00:08
                   */
                  class dem::records::Cell { 
                     
                     public:
                        
                        typedef dem::records::CellPacked Packed;
                        
                        enum State {
                           Leaf = 0, Refined = 1, Root = 2
                        };
                        
                        struct PersistentRecords {
                           bool _isInside;
                           State _state;
                           #ifdef UseManualAlignment
                           std::bitset<DIMENSIONS> _evenFlags __attribute__((aligned(VectorisationAlignment)));
                           #else
                           std::bitset<DIMENSIONS> _evenFlags;
                           #endif
                           #ifdef UseManualAlignment
                           tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber __attribute__((aligned(VectorisationAlignment)));
                           #else
                           tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
                           #endif
                           /**
                            * Generated
                            */
                           PersistentRecords();
                           
                           /**
                            * Generated
                            */
                           PersistentRecords(const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber);
                           
                           
                           inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _isInside;
                           }
                           
                           
                           
                           inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _isInside = isInside;
                           }
                           
                           
                           
                           inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _state;
                           }
                           
                           
                           
                           inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _state = state;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _evenFlags;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _evenFlags = (evenFlags);
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _accessNumber;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _accessNumber = (accessNumber);
                           }
                           
                           
                           
                        };
                        
                     private: 
                        PersistentRecords _persistentRecords;
                        
                     public:
                        /**
                         * Generated
                         */
                        Cell();
                        
                        /**
                         * Generated
                         */
                        Cell(const PersistentRecords& persistentRecords);
                        
                        /**
                         * Generated
                         */
                        Cell(const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber);
                        
                        /**
                         * Generated
                         */
                        virtual ~Cell();
                        
                        
                        inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._isInside;
                        }
                        
                        
                        
                        inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._isInside = isInside;
                        }
                        
                        
                        
                        inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._state;
                        }
                        
                        
                        
                        inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._state = state;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._evenFlags;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._evenFlags = (evenFlags);
                        }
                        
                        
                        
                        inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<DIMENSIONS);
                           return _persistentRecords._evenFlags[elementIndex];
                           
                        }
                        
                        
                        
                        inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<DIMENSIONS);
                           _persistentRecords._evenFlags[elementIndex]= evenFlags;
                           
                        }
                        
                        
                        
                        inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<DIMENSIONS);
                           _persistentRecords._evenFlags.flip(elementIndex);
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._accessNumber;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._accessNumber = (accessNumber);
                        }
                        
                        
                        
                        inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                           return _persistentRecords._accessNumber[elementIndex];
                           
                        }
                        
                        
                        
                        inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                           _persistentRecords._accessNumber[elementIndex]= accessNumber;
                           
                        }
                        
                        
                        /**
                         * Generated
                         */
                        static std::string toString(const State& param);
                        
                        /**
                         * Generated
                         */
                        static std::string getStateMapping();
                        
                        /**
                         * Generated
                         */
                        std::string toString() const;
                        
                        /**
                         * Generated
                         */
                        void toString(std::ostream& out) const;
                        
                        
                        PersistentRecords getPersistentRecords() const;
                        /**
                         * Generated
                         */
                        CellPacked convert() const;
                        
                        
                     #ifdef Parallel
                        protected:
                           static tarch::logging::Log _log;
                           
                           int _senderDestinationRank;
                           
                        public:
                           
                           /**
                            * Global that represents the mpi datatype.
                            * There are two variants: Datatype identifies only those attributes marked with
                            * parallelise. FullDatatype instead identifies the whole record with all fields.
                            */
                           static MPI_Datatype Datatype;
                           static MPI_Datatype FullDatatype;
                           
                           /**
                            * Initializes the data type for the mpi operations. Has to be called
                            * before the very first send or receive operation is called.
                            */
                           static void initDatatype();
                           
                           static void shutdownDatatype();
                           
                           /**
                            * @param communicateSleep -1 Data exchange through blocking mpi
                            * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
                            * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
                            */
                           void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                           
                           void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                           
                           static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                           
                           int getSenderRank() const;
                           
                     #endif
                        
                     };
                     
                     #ifndef DaStGenPackedPadding
                       #define DaStGenPackedPadding 1      // 32 bit version
                       // #define DaStGenPackedPadding 2   // 64 bit version
                     #endif
                     
                     
                     #ifdef PackedRecords
                        #pragma pack (push, DaStGenPackedPadding)
                     #endif
                     
                     /**
                      * @author This class is generated by DaStGen
                      * 		   DataStructureGenerator (DaStGen)
                      * 		   2007-2009 Wolfgang Eckhardt
                      * 		   2012      Tobias Weinzierl
                      *
                      * 		   build date: 09-02-2014 14:40
                      *
                      * @date   23/10/2016 00:08
                      */
                     class dem::records::CellPacked { 
                        
                        public:
                           
                           typedef dem::records::Cell::State State;
                           
                           struct PersistentRecords {
                              tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
                              
                              /** mapping of records:
                              || Member 	|| startbit 	|| length
                               |  isInside	| startbit 0	| #bits 1
                               |  state	| startbit 1	| #bits 2
                               |  evenFlags	| startbit 3	| #bits DIMENSIONS
                               */
                              short int _packedRecords0;
                              
                              /**
                               * Generated
                               */
                              PersistentRecords();
                              
                              /**
                               * Generated
                               */
                              PersistentRecords(const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber);
                              
                              
                              inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                              }
                              
                              
                              
                              inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 short int mask = 1 << (0);
   _packedRecords0 = static_cast<short int>( isInside ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                              }
                              
                              
                              
                              inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
                              }
                              
                              
                              
                              inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | static_cast<short int>(state) << (1));
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                                 mask = static_cast<short int>(mask << (3));
                                 short int tmp = static_cast<short int>(_packedRecords0 & mask);
                                 tmp = static_cast<short int>(tmp >> (3));
                                 std::bitset<DIMENSIONS> result = tmp;
                                 return result;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                                 mask = static_cast<short int>(mask << (3));
                                 _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
                                 _packedRecords0 = static_cast<short int>(_packedRecords0 | evenFlags.to_ulong() << (3));
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _accessNumber;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _accessNumber = (accessNumber);
                              }
                              
                              
                              
                           };
                           
                        private: 
                           PersistentRecords _persistentRecords;
                           
                        public:
                           /**
                            * Generated
                            */
                           CellPacked();
                           
                           /**
                            * Generated
                            */
                           CellPacked(const PersistentRecords& persistentRecords);
                           
                           /**
                            * Generated
                            */
                           CellPacked(const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber);
                           
                           /**
                            * Generated
                            */
                           virtual ~CellPacked();
                           
                           
                           inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                           }
                           
                           
                           
                           inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<short int>( isInside ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                           }
                           
                           
                           
                           inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
                           }
                           
                           
                           
                           inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | static_cast<short int>(state) << (1));
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                              mask = static_cast<short int>(mask << (3));
                              short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
                              tmp = static_cast<short int>(tmp >> (3));
                              std::bitset<DIMENSIONS> result = tmp;
                              return result;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                              mask = static_cast<short int>(mask << (3));
                              _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
                              _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | evenFlags.to_ulong() << (3));
                           }
                           
                           
                           
                           inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS);
                              int mask = 1 << (3);
                              mask = mask << elementIndex;
                              return (_persistentRecords._packedRecords0& mask);
                           }
                           
                           
                           
                           inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS);
                              assertion(!evenFlags || evenFlags==1);
                              int shift        = 3 + elementIndex; 
                              short int mask         = 1     << (shift);
                              short int shiftedValue = evenFlags << (shift);
                              _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 & ~mask;
                              _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 |  shiftedValue;
                           }
                           
                           
                           
                           inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS);
                              short int mask = 1 << (3);
                              mask = mask << elementIndex;
                              _persistentRecords._packedRecords0^= mask;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._accessNumber;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._accessNumber = (accessNumber);
                           }
                           
                           
                           
                           inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                              return _persistentRecords._accessNumber[elementIndex];
                              
                           }
                           
                           
                           
                           inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                              _persistentRecords._accessNumber[elementIndex]= accessNumber;
                              
                           }
                           
                           
                           /**
                            * Generated
                            */
                           static std::string toString(const State& param);
                           
                           /**
                            * Generated
                            */
                           static std::string getStateMapping();
                           
                           /**
                            * Generated
                            */
                           std::string toString() const;
                           
                           /**
                            * Generated
                            */
                           void toString(std::ostream& out) const;
                           
                           
                           PersistentRecords getPersistentRecords() const;
                           /**
                            * Generated
                            */
                           Cell convert() const;
                           
                           
                        #ifdef Parallel
                           protected:
                              static tarch::logging::Log _log;
                              
                              int _senderDestinationRank;
                              
                           public:
                              
                              /**
                               * Global that represents the mpi datatype.
                               * There are two variants: Datatype identifies only those attributes marked with
                               * parallelise. FullDatatype instead identifies the whole record with all fields.
                               */
                              static MPI_Datatype Datatype;
                              static MPI_Datatype FullDatatype;
                              
                              /**
                               * Initializes the data type for the mpi operations. Has to be called
                               * before the very first send or receive operation is called.
                               */
                              static void initDatatype();
                              
                              static void shutdownDatatype();
                              
                              /**
                               * @param communicateSleep -1 Data exchange through blocking mpi
                               * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
                               * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
                               */
                              void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                              
                              void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                              
                              static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                              
                              int getSenderRank() const;
                              
                        #endif
                           
                        };
                        
                        #ifdef PackedRecords
                        #pragma pack (pop)
                        #endif
                        
                        
                        
                     
                  #elif defined(Parallel) && !defined(Debug) && !defined(SharedMemoryParallelisation)
                     /**
                      * @author This class is generated by DaStGen
                      * 		   DataStructureGenerator (DaStGen)
                      * 		   2007-2009 Wolfgang Eckhardt
                      * 		   2012      Tobias Weinzierl
                      *
                      * 		   build date: 09-02-2014 14:40
                      *
                      * @date   23/10/2016 00:08
                      */
                     class dem::records::Cell { 
                        
                        public:
                           
                           typedef dem::records::CellPacked Packed;
                           
                           enum State {
                              Leaf = 0, Refined = 1, Root = 2
                           };
                           
                           struct PersistentRecords {
                              bool _isInside;
                              State _state;
                              #ifdef UseManualAlignment
                              std::bitset<DIMENSIONS> _evenFlags __attribute__((aligned(VectorisationAlignment)));
                              #else
                              std::bitset<DIMENSIONS> _evenFlags;
                              #endif
                              #ifdef UseManualAlignment
                              tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber __attribute__((aligned(VectorisationAlignment)));
                              #else
                              tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
                              #endif
                              int _responsibleRank;
                              bool _subtreeHoldsWorker;
                              bool _cellIsAForkCandidate;
                              /**
                               * Generated
                               */
                              PersistentRecords();
                              
                              /**
                               * Generated
                               */
                              PersistentRecords(const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const bool& cellIsAForkCandidate);
                              
                              
                              inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _isInside;
                              }
                              
                              
                              
                              inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _isInside = isInside;
                              }
                              
                              
                              
                              inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _state;
                              }
                              
                              
                              
                              inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _state = state;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _evenFlags;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _evenFlags = (evenFlags);
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _accessNumber;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _accessNumber = (accessNumber);
                              }
                              
                              
                              
                              inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _responsibleRank;
                              }
                              
                              
                              
                              inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _responsibleRank = responsibleRank;
                              }
                              
                              
                              
                              inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _subtreeHoldsWorker;
                              }
                              
                              
                              
                              inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _subtreeHoldsWorker = subtreeHoldsWorker;
                              }
                              
                              
                              
                              inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _cellIsAForkCandidate;
                              }
                              
                              
                              
                              inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _cellIsAForkCandidate = cellIsAForkCandidate;
                              }
                              
                              
                              
                           };
                           
                        private: 
                           PersistentRecords _persistentRecords;
                           
                        public:
                           /**
                            * Generated
                            */
                           Cell();
                           
                           /**
                            * Generated
                            */
                           Cell(const PersistentRecords& persistentRecords);
                           
                           /**
                            * Generated
                            */
                           Cell(const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const bool& cellIsAForkCandidate);
                           
                           /**
                            * Generated
                            */
                           virtual ~Cell();
                           
                           
                           inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._isInside;
                           }
                           
                           
                           
                           inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._isInside = isInside;
                           }
                           
                           
                           
                           inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._state;
                           }
                           
                           
                           
                           inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._state = state;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._evenFlags;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._evenFlags = (evenFlags);
                           }
                           
                           
                           
                           inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS);
                              return _persistentRecords._evenFlags[elementIndex];
                              
                           }
                           
                           
                           
                           inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS);
                              _persistentRecords._evenFlags[elementIndex]= evenFlags;
                              
                           }
                           
                           
                           
                           inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS);
                              _persistentRecords._evenFlags.flip(elementIndex);
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._accessNumber;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._accessNumber = (accessNumber);
                           }
                           
                           
                           
                           inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                              return _persistentRecords._accessNumber[elementIndex];
                              
                           }
                           
                           
                           
                           inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                              _persistentRecords._accessNumber[elementIndex]= accessNumber;
                              
                           }
                           
                           
                           
                           inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._responsibleRank;
                           }
                           
                           
                           
                           inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._responsibleRank = responsibleRank;
                           }
                           
                           
                           
                           inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._subtreeHoldsWorker;
                           }
                           
                           
                           
                           inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._subtreeHoldsWorker = subtreeHoldsWorker;
                           }
                           
                           
                           
                           inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._cellIsAForkCandidate;
                           }
                           
                           
                           
                           inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._cellIsAForkCandidate = cellIsAForkCandidate;
                           }
                           
                           
                           /**
                            * Generated
                            */
                           static std::string toString(const State& param);
                           
                           /**
                            * Generated
                            */
                           static std::string getStateMapping();
                           
                           /**
                            * Generated
                            */
                           std::string toString() const;
                           
                           /**
                            * Generated
                            */
                           void toString(std::ostream& out) const;
                           
                           
                           PersistentRecords getPersistentRecords() const;
                           /**
                            * Generated
                            */
                           CellPacked convert() const;
                           
                           
                        #ifdef Parallel
                           protected:
                              static tarch::logging::Log _log;
                              
                              int _senderDestinationRank;
                              
                           public:
                              
                              /**
                               * Global that represents the mpi datatype.
                               * There are two variants: Datatype identifies only those attributes marked with
                               * parallelise. FullDatatype instead identifies the whole record with all fields.
                               */
                              static MPI_Datatype Datatype;
                              static MPI_Datatype FullDatatype;
                              
                              /**
                               * Initializes the data type for the mpi operations. Has to be called
                               * before the very first send or receive operation is called.
                               */
                              static void initDatatype();
                              
                              static void shutdownDatatype();
                              
                              /**
                               * @param communicateSleep -1 Data exchange through blocking mpi
                               * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
                               * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
                               */
                              void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                              
                              void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                              
                              static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                              
                              int getSenderRank() const;
                              
                        #endif
                           
                        };
                        
                        #ifndef DaStGenPackedPadding
                          #define DaStGenPackedPadding 1      // 32 bit version
                          // #define DaStGenPackedPadding 2   // 64 bit version
                        #endif
                        
                        
                        #ifdef PackedRecords
                           #pragma pack (push, DaStGenPackedPadding)
                        #endif
                        
                        /**
                         * @author This class is generated by DaStGen
                         * 		   DataStructureGenerator (DaStGen)
                         * 		   2007-2009 Wolfgang Eckhardt
                         * 		   2012      Tobias Weinzierl
                         *
                         * 		   build date: 09-02-2014 14:40
                         *
                         * @date   23/10/2016 00:08
                         */
                        class dem::records::CellPacked { 
                           
                           public:
                              
                              typedef dem::records::Cell::State State;
                              
                              struct PersistentRecords {
                                 tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
                                 int _responsibleRank;
                                 bool _subtreeHoldsWorker;
                                 
                                 /** mapping of records:
                                 || Member 	|| startbit 	|| length
                                  |  isInside	| startbit 0	| #bits 1
                                  |  state	| startbit 1	| #bits 2
                                  |  evenFlags	| startbit 3	| #bits DIMENSIONS
                                  |  cellIsAForkCandidate	| startbit DIMENSIONS + 3	| #bits 1
                                  */
                                 short int _packedRecords0;
                                 
                                 /**
                                  * Generated
                                  */
                                 PersistentRecords();
                                 
                                 /**
                                  * Generated
                                  */
                                 PersistentRecords(const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const bool& cellIsAForkCandidate);
                                 
                                 
                                 inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                                 }
                                 
                                 
                                 
                                 inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    short int mask = 1 << (0);
   _packedRecords0 = static_cast<short int>( isInside ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                                 }
                                 
                                 
                                 
                                 inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
                                 }
                                 
                                 
                                 
                                 inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | static_cast<short int>(state) << (1));
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                                    mask = static_cast<short int>(mask << (3));
                                    short int tmp = static_cast<short int>(_packedRecords0 & mask);
                                    tmp = static_cast<short int>(tmp >> (3));
                                    std::bitset<DIMENSIONS> result = tmp;
                                    return result;
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                                    mask = static_cast<short int>(mask << (3));
                                    _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
                                    _packedRecords0 = static_cast<short int>(_packedRecords0 | evenFlags.to_ulong() << (3));
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _accessNumber;
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _accessNumber = (accessNumber);
                                 }
                                 
                                 
                                 
                                 inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _responsibleRank;
                                 }
                                 
                                 
                                 
                                 inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _responsibleRank = responsibleRank;
                                 }
                                 
                                 
                                 
                                 inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _subtreeHoldsWorker;
                                 }
                                 
                                 
                                 
                                 inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _subtreeHoldsWorker = subtreeHoldsWorker;
                                 }
                                 
                                 
                                 
                                 inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    short int mask = 1 << (DIMENSIONS + 3);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                                 }
                                 
                                 
                                 
                                 inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    short int mask = 1 << (DIMENSIONS + 3);
   _packedRecords0 = static_cast<short int>( cellIsAForkCandidate ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                                 }
                                 
                                 
                                 
                              };
                              
                           private: 
                              PersistentRecords _persistentRecords;
                              
                           public:
                              /**
                               * Generated
                               */
                              CellPacked();
                              
                              /**
                               * Generated
                               */
                              CellPacked(const PersistentRecords& persistentRecords);
                              
                              /**
                               * Generated
                               */
                              CellPacked(const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const bool& cellIsAForkCandidate);
                              
                              /**
                               * Generated
                               */
                              virtual ~CellPacked();
                              
                              
                              inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                              }
                              
                              
                              
                              inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 short int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<short int>( isInside ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                              }
                              
                              
                              
                              inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
                              }
                              
                              
                              
                              inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | static_cast<short int>(state) << (1));
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                                 mask = static_cast<short int>(mask << (3));
                                 short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
                                 tmp = static_cast<short int>(tmp >> (3));
                                 std::bitset<DIMENSIONS> result = tmp;
                                 return result;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                                 mask = static_cast<short int>(mask << (3));
                                 _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
                                 _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | evenFlags.to_ulong() << (3));
                              }
                              
                              
                              
                              inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 assertion(elementIndex>=0);
                                 assertion(elementIndex<DIMENSIONS);
                                 int mask = 1 << (3);
                                 mask = mask << elementIndex;
                                 return (_persistentRecords._packedRecords0& mask);
                              }
                              
                              
                              
                              inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 assertion(elementIndex>=0);
                                 assertion(elementIndex<DIMENSIONS);
                                 assertion(!evenFlags || evenFlags==1);
                                 int shift        = 3 + elementIndex; 
                                 short int mask         = 1     << (shift);
                                 short int shiftedValue = evenFlags << (shift);
                                 _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 & ~mask;
                                 _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 |  shiftedValue;
                              }
                              
                              
                              
                              inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 assertion(elementIndex>=0);
                                 assertion(elementIndex<DIMENSIONS);
                                 short int mask = 1 << (3);
                                 mask = mask << elementIndex;
                                 _persistentRecords._packedRecords0^= mask;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._accessNumber;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._accessNumber = (accessNumber);
                              }
                              
                              
                              
                              inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 assertion(elementIndex>=0);
                                 assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                 return _persistentRecords._accessNumber[elementIndex];
                                 
                              }
                              
                              
                              
                              inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 assertion(elementIndex>=0);
                                 assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                 _persistentRecords._accessNumber[elementIndex]= accessNumber;
                                 
                              }
                              
                              
                              
                              inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._responsibleRank;
                              }
                              
                              
                              
                              inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._responsibleRank = responsibleRank;
                              }
                              
                              
                              
                              inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._subtreeHoldsWorker;
                              }
                              
                              
                              
                              inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._subtreeHoldsWorker = subtreeHoldsWorker;
                              }
                              
                              
                              
                              inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 short int mask = 1 << (DIMENSIONS + 3);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                              }
                              
                              
                              
                              inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 short int mask = 1 << (DIMENSIONS + 3);
   _persistentRecords._packedRecords0 = static_cast<short int>( cellIsAForkCandidate ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                              }
                              
                              
                              /**
                               * Generated
                               */
                              static std::string toString(const State& param);
                              
                              /**
                               * Generated
                               */
                              static std::string getStateMapping();
                              
                              /**
                               * Generated
                               */
                              std::string toString() const;
                              
                              /**
                               * Generated
                               */
                              void toString(std::ostream& out) const;
                              
                              
                              PersistentRecords getPersistentRecords() const;
                              /**
                               * Generated
                               */
                              Cell convert() const;
                              
                              
                           #ifdef Parallel
                              protected:
                                 static tarch::logging::Log _log;
                                 
                                 int _senderDestinationRank;
                                 
                              public:
                                 
                                 /**
                                  * Global that represents the mpi datatype.
                                  * There are two variants: Datatype identifies only those attributes marked with
                                  * parallelise. FullDatatype instead identifies the whole record with all fields.
                                  */
                                 static MPI_Datatype Datatype;
                                 static MPI_Datatype FullDatatype;
                                 
                                 /**
                                  * Initializes the data type for the mpi operations. Has to be called
                                  * before the very first send or receive operation is called.
                                  */
                                 static void initDatatype();
                                 
                                 static void shutdownDatatype();
                                 
                                 /**
                                  * @param communicateSleep -1 Data exchange through blocking mpi
                                  * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
                                  * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
                                  */
                                 void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                                 
                                 void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                                 
                                 static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                                 
                                 int getSenderRank() const;
                                 
                           #endif
                              
                           };
                           
                           #ifdef PackedRecords
                           #pragma pack (pop)
                           #endif
                           
                           
                           
                        
                     #elif defined(Debug) && !defined(Parallel) && !defined(SharedMemoryParallelisation)
                        /**
                         * @author This class is generated by DaStGen
                         * 		   DataStructureGenerator (DaStGen)
                         * 		   2007-2009 Wolfgang Eckhardt
                         * 		   2012      Tobias Weinzierl
                         *
                         * 		   build date: 09-02-2014 14:40
                         *
                         * @date   23/10/2016 00:08
                         */
                        class dem::records::Cell { 
                           
                           public:
                              
                              typedef dem::records::CellPacked Packed;
                              
                              enum State {
                                 Leaf = 0, Refined = 1, Root = 2
                              };
                              
                              struct PersistentRecords {
                                 bool _isInside;
                                 State _state;
                                 int _level;
                                 #ifdef UseManualAlignment
                                 std::bitset<DIMENSIONS> _evenFlags __attribute__((aligned(VectorisationAlignment)));
                                 #else
                                 std::bitset<DIMENSIONS> _evenFlags;
                                 #endif
                                 #ifdef UseManualAlignment
                                 tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber __attribute__((aligned(VectorisationAlignment)));
                                 #else
                                 tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
                                 #endif
                                 /**
                                  * Generated
                                  */
                                 PersistentRecords();
                                 
                                 /**
                                  * Generated
                                  */
                                 PersistentRecords(const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber);
                                 
                                 
                                 inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _isInside;
                                 }
                                 
                                 
                                 
                                 inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _isInside = isInside;
                                 }
                                 
                                 
                                 
                                 inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _state;
                                 }
                                 
                                 
                                 
                                 inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _state = state;
                                 }
                                 
                                 
                                 
                                 inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _level;
                                 }
                                 
                                 
                                 
                                 inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _level = level;
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _evenFlags;
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _evenFlags = (evenFlags);
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _accessNumber;
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _accessNumber = (accessNumber);
                                 }
                                 
                                 
                                 
                              };
                              
                           private: 
                              PersistentRecords _persistentRecords;
                              
                           public:
                              /**
                               * Generated
                               */
                              Cell();
                              
                              /**
                               * Generated
                               */
                              Cell(const PersistentRecords& persistentRecords);
                              
                              /**
                               * Generated
                               */
                              Cell(const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber);
                              
                              /**
                               * Generated
                               */
                              virtual ~Cell();
                              
                              
                              inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._isInside;
                              }
                              
                              
                              
                              inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._isInside = isInside;
                              }
                              
                              
                              
                              inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._state;
                              }
                              
                              
                              
                              inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._state = state;
                              }
                              
                              
                              
                              inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._level;
                              }
                              
                              
                              
                              inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._level = level;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._evenFlags;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._evenFlags = (evenFlags);
                              }
                              
                              
                              
                              inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 assertion(elementIndex>=0);
                                 assertion(elementIndex<DIMENSIONS);
                                 return _persistentRecords._evenFlags[elementIndex];
                                 
                              }
                              
                              
                              
                              inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 assertion(elementIndex>=0);
                                 assertion(elementIndex<DIMENSIONS);
                                 _persistentRecords._evenFlags[elementIndex]= evenFlags;
                                 
                              }
                              
                              
                              
                              inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 assertion(elementIndex>=0);
                                 assertion(elementIndex<DIMENSIONS);
                                 _persistentRecords._evenFlags.flip(elementIndex);
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._accessNumber;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._accessNumber = (accessNumber);
                              }
                              
                              
                              
                              inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 assertion(elementIndex>=0);
                                 assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                 return _persistentRecords._accessNumber[elementIndex];
                                 
                              }
                              
                              
                              
                              inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 assertion(elementIndex>=0);
                                 assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                 _persistentRecords._accessNumber[elementIndex]= accessNumber;
                                 
                              }
                              
                              
                              /**
                               * Generated
                               */
                              static std::string toString(const State& param);
                              
                              /**
                               * Generated
                               */
                              static std::string getStateMapping();
                              
                              /**
                               * Generated
                               */
                              std::string toString() const;
                              
                              /**
                               * Generated
                               */
                              void toString(std::ostream& out) const;
                              
                              
                              PersistentRecords getPersistentRecords() const;
                              /**
                               * Generated
                               */
                              CellPacked convert() const;
                              
                              
                           #ifdef Parallel
                              protected:
                                 static tarch::logging::Log _log;
                                 
                                 int _senderDestinationRank;
                                 
                              public:
                                 
                                 /**
                                  * Global that represents the mpi datatype.
                                  * There are two variants: Datatype identifies only those attributes marked with
                                  * parallelise. FullDatatype instead identifies the whole record with all fields.
                                  */
                                 static MPI_Datatype Datatype;
                                 static MPI_Datatype FullDatatype;
                                 
                                 /**
                                  * Initializes the data type for the mpi operations. Has to be called
                                  * before the very first send or receive operation is called.
                                  */
                                 static void initDatatype();
                                 
                                 static void shutdownDatatype();
                                 
                                 /**
                                  * @param communicateSleep -1 Data exchange through blocking mpi
                                  * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
                                  * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
                                  */
                                 void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                                 
                                 void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                                 
                                 static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                                 
                                 int getSenderRank() const;
                                 
                           #endif
                              
                           };
                           
                           #ifndef DaStGenPackedPadding
                             #define DaStGenPackedPadding 1      // 32 bit version
                             // #define DaStGenPackedPadding 2   // 64 bit version
                           #endif
                           
                           
                           #ifdef PackedRecords
                              #pragma pack (push, DaStGenPackedPadding)
                           #endif
                           
                           /**
                            * @author This class is generated by DaStGen
                            * 		   DataStructureGenerator (DaStGen)
                            * 		   2007-2009 Wolfgang Eckhardt
                            * 		   2012      Tobias Weinzierl
                            *
                            * 		   build date: 09-02-2014 14:40
                            *
                            * @date   23/10/2016 00:08
                            */
                           class dem::records::CellPacked { 
                              
                              public:
                                 
                                 typedef dem::records::Cell::State State;
                                 
                                 struct PersistentRecords {
                                    int _level;
                                    tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
                                    
                                    /** mapping of records:
                                    || Member 	|| startbit 	|| length
                                     |  isInside	| startbit 0	| #bits 1
                                     |  state	| startbit 1	| #bits 2
                                     |  evenFlags	| startbit 3	| #bits DIMENSIONS
                                     */
                                    short int _packedRecords0;
                                    
                                    /**
                                     * Generated
                                     */
                                    PersistentRecords();
                                    
                                    /**
                                     * Generated
                                     */
                                    PersistentRecords(const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber);
                                    
                                    
                                    inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                                    }
                                    
                                    
                                    
                                    inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       short int mask = 1 << (0);
   _packedRecords0 = static_cast<short int>( isInside ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                                    }
                                    
                                    
                                    
                                    inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
                                    }
                                    
                                    
                                    
                                    inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | static_cast<short int>(state) << (1));
                                    }
                                    
                                    
                                    
                                    inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _level;
                                    }
                                    
                                    
                                    
                                    inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _level = level;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                                       mask = static_cast<short int>(mask << (3));
                                       short int tmp = static_cast<short int>(_packedRecords0 & mask);
                                       tmp = static_cast<short int>(tmp >> (3));
                                       std::bitset<DIMENSIONS> result = tmp;
                                       return result;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                                       mask = static_cast<short int>(mask << (3));
                                       _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
                                       _packedRecords0 = static_cast<short int>(_packedRecords0 | evenFlags.to_ulong() << (3));
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _accessNumber;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _accessNumber = (accessNumber);
                                    }
                                    
                                    
                                    
                                 };
                                 
                              private: 
                                 PersistentRecords _persistentRecords;
                                 
                              public:
                                 /**
                                  * Generated
                                  */
                                 CellPacked();
                                 
                                 /**
                                  * Generated
                                  */
                                 CellPacked(const PersistentRecords& persistentRecords);
                                 
                                 /**
                                  * Generated
                                  */
                                 CellPacked(const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber);
                                 
                                 /**
                                  * Generated
                                  */
                                 virtual ~CellPacked();
                                 
                                 
                                 inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                                 }
                                 
                                 
                                 
                                 inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    short int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<short int>( isInside ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                                 }
                                 
                                 
                                 
                                 inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
                                 }
                                 
                                 
                                 
                                 inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | static_cast<short int>(state) << (1));
                                 }
                                 
                                 
                                 
                                 inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _persistentRecords._level;
                                 }
                                 
                                 
                                 
                                 inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _persistentRecords._level = level;
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                                    mask = static_cast<short int>(mask << (3));
                                    short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
                                    tmp = static_cast<short int>(tmp >> (3));
                                    std::bitset<DIMENSIONS> result = tmp;
                                    return result;
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                                    mask = static_cast<short int>(mask << (3));
                                    _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
                                    _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | evenFlags.to_ulong() << (3));
                                 }
                                 
                                 
                                 
                                 inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    assertion(elementIndex>=0);
                                    assertion(elementIndex<DIMENSIONS);
                                    int mask = 1 << (3);
                                    mask = mask << elementIndex;
                                    return (_persistentRecords._packedRecords0& mask);
                                 }
                                 
                                 
                                 
                                 inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    assertion(elementIndex>=0);
                                    assertion(elementIndex<DIMENSIONS);
                                    assertion(!evenFlags || evenFlags==1);
                                    int shift        = 3 + elementIndex; 
                                    short int mask         = 1     << (shift);
                                    short int shiftedValue = evenFlags << (shift);
                                    _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 & ~mask;
                                    _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 |  shiftedValue;
                                 }
                                 
                                 
                                 
                                 inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    assertion(elementIndex>=0);
                                    assertion(elementIndex<DIMENSIONS);
                                    short int mask = 1 << (3);
                                    mask = mask << elementIndex;
                                    _persistentRecords._packedRecords0^= mask;
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _persistentRecords._accessNumber;
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _persistentRecords._accessNumber = (accessNumber);
                                 }
                                 
                                 
                                 
                                 inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    assertion(elementIndex>=0);
                                    assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                    return _persistentRecords._accessNumber[elementIndex];
                                    
                                 }
                                 
                                 
                                 
                                 inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    assertion(elementIndex>=0);
                                    assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                    _persistentRecords._accessNumber[elementIndex]= accessNumber;
                                    
                                 }
                                 
                                 
                                 /**
                                  * Generated
                                  */
                                 static std::string toString(const State& param);
                                 
                                 /**
                                  * Generated
                                  */
                                 static std::string getStateMapping();
                                 
                                 /**
                                  * Generated
                                  */
                                 std::string toString() const;
                                 
                                 /**
                                  * Generated
                                  */
                                 void toString(std::ostream& out) const;
                                 
                                 
                                 PersistentRecords getPersistentRecords() const;
                                 /**
                                  * Generated
                                  */
                                 Cell convert() const;
                                 
                                 
                              #ifdef Parallel
                                 protected:
                                    static tarch::logging::Log _log;
                                    
                                    int _senderDestinationRank;
                                    
                                 public:
                                    
                                    /**
                                     * Global that represents the mpi datatype.
                                     * There are two variants: Datatype identifies only those attributes marked with
                                     * parallelise. FullDatatype instead identifies the whole record with all fields.
                                     */
                                    static MPI_Datatype Datatype;
                                    static MPI_Datatype FullDatatype;
                                    
                                    /**
                                     * Initializes the data type for the mpi operations. Has to be called
                                     * before the very first send or receive operation is called.
                                     */
                                    static void initDatatype();
                                    
                                    static void shutdownDatatype();
                                    
                                    /**
                                     * @param communicateSleep -1 Data exchange through blocking mpi
                                     * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
                                     * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
                                     */
                                    void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                                    
                                    void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                                    
                                    static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                                    
                                    int getSenderRank() const;
                                    
                              #endif
                                 
                              };
                              
                              #ifdef PackedRecords
                              #pragma pack (pop)
                              #endif
                              
                              
                              
                           
                        #elif !defined(Parallel) && !defined(Debug) && defined(SharedMemoryParallelisation)
                           /**
                            * @author This class is generated by DaStGen
                            * 		   DataStructureGenerator (DaStGen)
                            * 		   2007-2009 Wolfgang Eckhardt
                            * 		   2012      Tobias Weinzierl
                            *
                            * 		   build date: 09-02-2014 14:40
                            *
                            * @date   23/10/2016 00:08
                            */
                           class dem::records::Cell { 
                              
                              public:
                                 
                                 typedef dem::records::CellPacked Packed;
                                 
                                 enum State {
                                    Leaf = 0, Refined = 1, Root = 2
                                 };
                                 
                                 struct PersistentRecords {
                                    bool _isInside;
                                    State _state;
                                    #ifdef UseManualAlignment
                                    std::bitset<DIMENSIONS> _evenFlags __attribute__((aligned(VectorisationAlignment)));
                                    #else
                                    std::bitset<DIMENSIONS> _evenFlags;
                                    #endif
                                    #ifdef UseManualAlignment
                                    tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber __attribute__((aligned(VectorisationAlignment)));
                                    #else
                                    tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
                                    #endif
                                    int _numberOfLoadsFromInputStream;
                                    int _numberOfStoresToOutputStream;
                                    /**
                                     * Generated
                                     */
                                    PersistentRecords();
                                    
                                    /**
                                     * Generated
                                     */
                                    PersistentRecords(const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
                                    
                                    
                                    inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _isInside;
                                    }
                                    
                                    
                                    
                                    inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _isInside = isInside;
                                    }
                                    
                                    
                                    
                                    inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _state;
                                    }
                                    
                                    
                                    
                                    inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _state = state;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _evenFlags;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _evenFlags = (evenFlags);
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _accessNumber;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _accessNumber = (accessNumber);
                                    }
                                    
                                    
                                    
                                    inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _numberOfLoadsFromInputStream;
                                    }
                                    
                                    
                                    
                                    inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
                                    }
                                    
                                    
                                    
                                    inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _numberOfStoresToOutputStream;
                                    }
                                    
                                    
                                    
                                    inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _numberOfStoresToOutputStream = numberOfStoresToOutputStream;
                                    }
                                    
                                    
                                    
                                 };
                                 
                              private: 
                                 PersistentRecords _persistentRecords;
                                 
                              public:
                                 /**
                                  * Generated
                                  */
                                 Cell();
                                 
                                 /**
                                  * Generated
                                  */
                                 Cell(const PersistentRecords& persistentRecords);
                                 
                                 /**
                                  * Generated
                                  */
                                 Cell(const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
                                 
                                 /**
                                  * Generated
                                  */
                                 virtual ~Cell();
                                 
                                 
                                 inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _persistentRecords._isInside;
                                 }
                                 
                                 
                                 
                                 inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _persistentRecords._isInside = isInside;
                                 }
                                 
                                 
                                 
                                 inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _persistentRecords._state;
                                 }
                                 
                                 
                                 
                                 inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _persistentRecords._state = state;
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _persistentRecords._evenFlags;
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _persistentRecords._evenFlags = (evenFlags);
                                 }
                                 
                                 
                                 
                                 inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    assertion(elementIndex>=0);
                                    assertion(elementIndex<DIMENSIONS);
                                    return _persistentRecords._evenFlags[elementIndex];
                                    
                                 }
                                 
                                 
                                 
                                 inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    assertion(elementIndex>=0);
                                    assertion(elementIndex<DIMENSIONS);
                                    _persistentRecords._evenFlags[elementIndex]= evenFlags;
                                    
                                 }
                                 
                                 
                                 
                                 inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    assertion(elementIndex>=0);
                                    assertion(elementIndex<DIMENSIONS);
                                    _persistentRecords._evenFlags.flip(elementIndex);
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _persistentRecords._accessNumber;
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _persistentRecords._accessNumber = (accessNumber);
                                 }
                                 
                                 
                                 
                                 inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    assertion(elementIndex>=0);
                                    assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                    return _persistentRecords._accessNumber[elementIndex];
                                    
                                 }
                                 
                                 
                                 
                                 inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    assertion(elementIndex>=0);
                                    assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                    _persistentRecords._accessNumber[elementIndex]= accessNumber;
                                    
                                 }
                                 
                                 
                                 
                                 inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _persistentRecords._numberOfLoadsFromInputStream;
                                 }
                                 
                                 
                                 
                                 inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _persistentRecords._numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
                                 }
                                 
                                 
                                 
                                 inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _persistentRecords._numberOfStoresToOutputStream;
                                 }
                                 
                                 
                                 
                                 inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _persistentRecords._numberOfStoresToOutputStream = numberOfStoresToOutputStream;
                                 }
                                 
                                 
                                 /**
                                  * Generated
                                  */
                                 static std::string toString(const State& param);
                                 
                                 /**
                                  * Generated
                                  */
                                 static std::string getStateMapping();
                                 
                                 /**
                                  * Generated
                                  */
                                 std::string toString() const;
                                 
                                 /**
                                  * Generated
                                  */
                                 void toString(std::ostream& out) const;
                                 
                                 
                                 PersistentRecords getPersistentRecords() const;
                                 /**
                                  * Generated
                                  */
                                 CellPacked convert() const;
                                 
                                 
                              #ifdef Parallel
                                 protected:
                                    static tarch::logging::Log _log;
                                    
                                    int _senderDestinationRank;
                                    
                                 public:
                                    
                                    /**
                                     * Global that represents the mpi datatype.
                                     * There are two variants: Datatype identifies only those attributes marked with
                                     * parallelise. FullDatatype instead identifies the whole record with all fields.
                                     */
                                    static MPI_Datatype Datatype;
                                    static MPI_Datatype FullDatatype;
                                    
                                    /**
                                     * Initializes the data type for the mpi operations. Has to be called
                                     * before the very first send or receive operation is called.
                                     */
                                    static void initDatatype();
                                    
                                    static void shutdownDatatype();
                                    
                                    /**
                                     * @param communicateSleep -1 Data exchange through blocking mpi
                                     * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
                                     * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
                                     */
                                    void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                                    
                                    void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                                    
                                    static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                                    
                                    int getSenderRank() const;
                                    
                              #endif
                                 
                              };
                              
                              #ifndef DaStGenPackedPadding
                                #define DaStGenPackedPadding 1      // 32 bit version
                                // #define DaStGenPackedPadding 2   // 64 bit version
                              #endif
                              
                              
                              #ifdef PackedRecords
                                 #pragma pack (push, DaStGenPackedPadding)
                              #endif
                              
                              /**
                               * @author This class is generated by DaStGen
                               * 		   DataStructureGenerator (DaStGen)
                               * 		   2007-2009 Wolfgang Eckhardt
                               * 		   2012      Tobias Weinzierl
                               *
                               * 		   build date: 09-02-2014 14:40
                               *
                               * @date   23/10/2016 00:08
                               */
                              class dem::records::CellPacked { 
                                 
                                 public:
                                    
                                    typedef dem::records::Cell::State State;
                                    
                                    struct PersistentRecords {
                                       tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
                                       int _numberOfLoadsFromInputStream;
                                       int _numberOfStoresToOutputStream;
                                       
                                       /** mapping of records:
                                       || Member 	|| startbit 	|| length
                                        |  isInside	| startbit 0	| #bits 1
                                        |  state	| startbit 1	| #bits 2
                                        |  evenFlags	| startbit 3	| #bits DIMENSIONS
                                        */
                                       short int _packedRecords0;
                                       
                                       /**
                                        * Generated
                                        */
                                       PersistentRecords();
                                       
                                       /**
                                        * Generated
                                        */
                                       PersistentRecords(const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
                                       
                                       
                                       inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                                       }
                                       
                                       
                                       
                                       inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          short int mask = 1 << (0);
   _packedRecords0 = static_cast<short int>( isInside ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                                       }
                                       
                                       
                                       
                                       inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
                                       }
                                       
                                       
                                       
                                       inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | static_cast<short int>(state) << (1));
                                       }
                                       
                                       
                                       
                                       /**
                                        * Generated and optimized
                                        * 
                                        * If you realise a for loop using exclusively arrays (vectors) and compile 
                                        * with -DUseManualAlignment you may add 
                                        * \code
                                        #pragma vector aligned
                                        #pragma simd
                                        \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                        * 
                                        * The alignment is tied to the unpacked records, i.e. for packed class
                                        * variants the machine's natural alignment is switched off to recude the  
                                        * memory footprint. Do not use any SSE/AVX operations or 
                                        * vectorisation on the result for the packed variants, as the data is misaligned. 
                                        * If you rely on vectorisation, convert the underlying record 
                                        * into the unpacked version first. 
                                        * 
                                        * @see convert()
                                        */
                                       inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                                          mask = static_cast<short int>(mask << (3));
                                          short int tmp = static_cast<short int>(_packedRecords0 & mask);
                                          tmp = static_cast<short int>(tmp >> (3));
                                          std::bitset<DIMENSIONS> result = tmp;
                                          return result;
                                       }
                                       
                                       
                                       
                                       /**
                                        * Generated and optimized
                                        * 
                                        * If you realise a for loop using exclusively arrays (vectors) and compile 
                                        * with -DUseManualAlignment you may add 
                                        * \code
                                        #pragma vector aligned
                                        #pragma simd
                                        \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                        * 
                                        * The alignment is tied to the unpacked records, i.e. for packed class
                                        * variants the machine's natural alignment is switched off to recude the  
                                        * memory footprint. Do not use any SSE/AVX operations or 
                                        * vectorisation on the result for the packed variants, as the data is misaligned. 
                                        * If you rely on vectorisation, convert the underlying record 
                                        * into the unpacked version first. 
                                        * 
                                        * @see convert()
                                        */
                                       inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                                          mask = static_cast<short int>(mask << (3));
                                          _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
                                          _packedRecords0 = static_cast<short int>(_packedRecords0 | evenFlags.to_ulong() << (3));
                                       }
                                       
                                       
                                       
                                       /**
                                        * Generated and optimized
                                        * 
                                        * If you realise a for loop using exclusively arrays (vectors) and compile 
                                        * with -DUseManualAlignment you may add 
                                        * \code
                                        #pragma vector aligned
                                        #pragma simd
                                        \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                        * 
                                        * The alignment is tied to the unpacked records, i.e. for packed class
                                        * variants the machine's natural alignment is switched off to recude the  
                                        * memory footprint. Do not use any SSE/AVX operations or 
                                        * vectorisation on the result for the packed variants, as the data is misaligned. 
                                        * If you rely on vectorisation, convert the underlying record 
                                        * into the unpacked version first. 
                                        * 
                                        * @see convert()
                                        */
                                       inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _accessNumber;
                                       }
                                       
                                       
                                       
                                       /**
                                        * Generated and optimized
                                        * 
                                        * If you realise a for loop using exclusively arrays (vectors) and compile 
                                        * with -DUseManualAlignment you may add 
                                        * \code
                                        #pragma vector aligned
                                        #pragma simd
                                        \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                        * 
                                        * The alignment is tied to the unpacked records, i.e. for packed class
                                        * variants the machine's natural alignment is switched off to recude the  
                                        * memory footprint. Do not use any SSE/AVX operations or 
                                        * vectorisation on the result for the packed variants, as the data is misaligned. 
                                        * If you rely on vectorisation, convert the underlying record 
                                        * into the unpacked version first. 
                                        * 
                                        * @see convert()
                                        */
                                       inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _accessNumber = (accessNumber);
                                       }
                                       
                                       
                                       
                                       inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _numberOfLoadsFromInputStream;
                                       }
                                       
                                       
                                       
                                       inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
                                       }
                                       
                                       
                                       
                                       inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _numberOfStoresToOutputStream;
                                       }
                                       
                                       
                                       
                                       inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _numberOfStoresToOutputStream = numberOfStoresToOutputStream;
                                       }
                                       
                                       
                                       
                                    };
                                    
                                 private: 
                                    PersistentRecords _persistentRecords;
                                    
                                 public:
                                    /**
                                     * Generated
                                     */
                                    CellPacked();
                                    
                                    /**
                                     * Generated
                                     */
                                    CellPacked(const PersistentRecords& persistentRecords);
                                    
                                    /**
                                     * Generated
                                     */
                                    CellPacked(const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
                                    
                                    /**
                                     * Generated
                                     */
                                    virtual ~CellPacked();
                                    
                                    
                                    inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                                    }
                                    
                                    
                                    
                                    inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       short int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<short int>( isInside ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                                    }
                                    
                                    
                                    
                                    inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
                                    }
                                    
                                    
                                    
                                    inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | static_cast<short int>(state) << (1));
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                                       mask = static_cast<short int>(mask << (3));
                                       short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
                                       tmp = static_cast<short int>(tmp >> (3));
                                       std::bitset<DIMENSIONS> result = tmp;
                                       return result;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                                       mask = static_cast<short int>(mask << (3));
                                       _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
                                       _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | evenFlags.to_ulong() << (3));
                                    }
                                    
                                    
                                    
                                    inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS);
                                       int mask = 1 << (3);
                                       mask = mask << elementIndex;
                                       return (_persistentRecords._packedRecords0& mask);
                                    }
                                    
                                    
                                    
                                    inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS);
                                       assertion(!evenFlags || evenFlags==1);
                                       int shift        = 3 + elementIndex; 
                                       short int mask         = 1     << (shift);
                                       short int shiftedValue = evenFlags << (shift);
                                       _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 & ~mask;
                                       _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 |  shiftedValue;
                                    }
                                    
                                    
                                    
                                    inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS);
                                       short int mask = 1 << (3);
                                       mask = mask << elementIndex;
                                       _persistentRecords._packedRecords0^= mask;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._accessNumber;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._accessNumber = (accessNumber);
                                    }
                                    
                                    
                                    
                                    inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                       return _persistentRecords._accessNumber[elementIndex];
                                       
                                    }
                                    
                                    
                                    
                                    inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                       _persistentRecords._accessNumber[elementIndex]= accessNumber;
                                       
                                    }
                                    
                                    
                                    
                                    inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._numberOfLoadsFromInputStream;
                                    }
                                    
                                    
                                    
                                    inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
                                    }
                                    
                                    
                                    
                                    inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._numberOfStoresToOutputStream;
                                    }
                                    
                                    
                                    
                                    inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._numberOfStoresToOutputStream = numberOfStoresToOutputStream;
                                    }
                                    
                                    
                                    /**
                                     * Generated
                                     */
                                    static std::string toString(const State& param);
                                    
                                    /**
                                     * Generated
                                     */
                                    static std::string getStateMapping();
                                    
                                    /**
                                     * Generated
                                     */
                                    std::string toString() const;
                                    
                                    /**
                                     * Generated
                                     */
                                    void toString(std::ostream& out) const;
                                    
                                    
                                    PersistentRecords getPersistentRecords() const;
                                    /**
                                     * Generated
                                     */
                                    Cell convert() const;
                                    
                                    
                                 #ifdef Parallel
                                    protected:
                                       static tarch::logging::Log _log;
                                       
                                       int _senderDestinationRank;
                                       
                                    public:
                                       
                                       /**
                                        * Global that represents the mpi datatype.
                                        * There are two variants: Datatype identifies only those attributes marked with
                                        * parallelise. FullDatatype instead identifies the whole record with all fields.
                                        */
                                       static MPI_Datatype Datatype;
                                       static MPI_Datatype FullDatatype;
                                       
                                       /**
                                        * Initializes the data type for the mpi operations. Has to be called
                                        * before the very first send or receive operation is called.
                                        */
                                       static void initDatatype();
                                       
                                       static void shutdownDatatype();
                                       
                                       /**
                                        * @param communicateSleep -1 Data exchange through blocking mpi
                                        * @param communicateSleep  0 Data exchange through non-blocking mpi, i.e. pending messages are received via polling until MPI_Test succeeds
                                        * @param communicateSleep >0 Same as 0 but in addition, each unsuccessful MPI_Test is follows by an usleep
                                        */
                                       void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                                       
                                       void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise, int communicateSleep);
                                       
                                       static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                                       
                                       int getSenderRank() const;
                                       
                                 #endif
                                    
                                 };
                                 
                                 #ifdef PackedRecords
                                 #pragma pack (pop)
                                 #endif
                                 
                                 
                                 
                              
                           #endif
                           
                           #endif
                           
