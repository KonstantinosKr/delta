\section{Shared memory parallelisation}
\label{section:shared-memory}

We introduce three levels of shared memory parallelisation. On the highest level, we exploit the cell level decomposition and assign work within each cell to cores. Within the cell we assign work per particle pair  to an inner fork-level of threads. Lastly within each particle pair the innermost level of parallelisation is utilised to exploit tessellation level parallelisation by the contact solver.

Three levels of multicore parallelisation:

a) on the cell level
b) within one 'cell' do all the particles in parallel
c) within one particle-particle sweep, do all the triangles in parallel

%describe the setup
For the multi-core scaling experiments we setup runs that use the hybrid solver and the brute force solver. We base performance experiments on three main runtime setups. We use meshed non-spherical particles of approximately 60 triangles each based on the Delauny triangulation (see appendix) derived from a 50 sphere shaped point cloud. We scale the contact problem in two aspects; by number of non-spherical particles and by irregularity of particle radius size. We use regular grid, adaptive and reluctant grids for grid-based adaptivity. Lastly, the particles mass is homogeneous to the particles and the initial velocities are random for every particle. To enhance the time to solution we use spheres as a last stage boundary box to minimize the number of mesh to mesh contact solving per timestep per cell per vertex.

\clearpage

In the innermost level of parallesation we are experimenting with several solver methods to resolve contact points. At this level computation is parallelised based upon triangle pairs and triangle batches and assigned to threads statically. 
From the performance measurements in Figure \ref{figure:triangle_omp} it is observed that Hybrid-on-triangle-pairs scales but it is slowest method to solution per timestep. 

\begin{figure}[htb]
  \begin{center}
    \includegraphics[width=0.8\textwidth]{experiments/random/omp/triangle_based_x0.png}
  \end{center}
  \caption{Triangle based shared memory parallelism using openMP running brute force (bf), hybrid-on-batches (HBatches) and hybrid-on-triangle-pairs (HTriangles) methods.}
  \label{figure:triangle_omp}
\end{figure}

\begin{figure}[htb]
  \begin{center}
    \includegraphics[width=0.8\textwidth]{experiments/random/omp/particle_based_x0.png}
  \end{center}
  \caption{Particle based shared memory parallelism using openMP running brute force (bf), hybrid-on-batches (HBatches) and hybrid-on-triangle-pairs (HTriangles) methods.}
  \label{figure:particle_omp}
\end{figure}

The alternative method of parallelisation is based on whole particle pairs. Thread subscription is performed during the outer sweeping of particles where pairs-of-particles per grid-vertex are computed in parallel on each vertex visit. In Figure \ref{figure:particle_omp} we see that again hybrid-on-triangle-pairs is performing worst and hybrid-on-batches to scale better. Brute force does not scale as well as hybrid-on-batches but has faster time to solution on low number of threads.

\begin{figure}[htb]
  \begin{center}
    \includegraphics[width=1\textwidth]{experiments/random/omp/particle_triangle_based_x0.png}
  \end{center}
  \caption{Particle and Triangle based nested shared memory parallelism using openMP running brute force (bf), hybrid-on-batches (HBatches) and hybrid-on-triangle-pairs (HTriangles) methods.}
  \label{figure:particletriangle_omp}
\end{figure}

Particle and Triangle parallelism nests both particle and triangle shared memory threads. The runtime to solution is slightly slower than particle-based parallelism due to the thread/openMP overhead. Nevertheless it has an impact on brute-force method as it scales smoother than particle-based only parallelism due to overhead. In this case hybrid-on-batches is also the fastest while the hybrid-on-triangle-pairs the slowest. 

\clearpage

\begin{figure}[htb]
  \begin{center}
    \includegraphics[width=1\textwidth]{experiments/random/omp/tbb_vs_serial.png}
  \end{center}
  \caption{Cell based parallelism on Peano compared to serial runs using Intel TBB}
  \label{figure:tbb_vs_serial}
\end{figure}

Another aspect of shared memory performance is thread scheduling and balancing implications.
The non-deterministic nature and error-prone distribution of triangles of the hybrid algorithm as discussed in Chapter {-hybrid chapter-} would suggest that alternative scheduling would pay off. But for our experiments dynamic and guided thread scheduling didn't not have a performance improvement on our tests but they rather create an scheduling overhead instead. That is partly due to the distribution of error in the triangle pairs and due to the granularity of our batches. Insignificant improvements were shown to our worst hybrid-on-triangle-pair method when using dynamic scheduling with OpenMP on the ivy bridge machine.    

At the highest level of shared memory parallelism is the grid cell-based multicore processing. It is based on the peano-framework that is using Intel TBBs to assign cells on threads. This has significant impact on the overall runtime performance in Figure \ref{figure:tbb_vs_serial} it is compared with plain serial execution. Number of threads on the x axis refer to TBB Cell-based threads, at the contact detection method level the computation is performed without shared memory parallelism but with vectorization enabled. For the specified experiment in Figure {} we observe no cell-based thread scaling but only reduction of time to solution. But when we increase the problem size further (figure \ref{figure:tbb_scaling}) we see that cell-based parallelism scales and it enhances execution time. 

\begin{figure}[htb]
  \begin{center}
    \includegraphics[width=0.8\textwidth]{experiments/random/omp/tbb_regular_x2.png}
  \end{center}
  \caption{Cell based parallelism on Peano compared to serial runs using Intel TBB}
  \label{figure:tbb_scaling}
\end{figure}

Overall shared memory parallelism on these three levels of parallelisation yield performance gain on manycore systems for our hybrid-on-batches method and in some cases brute force. For small problems triangle-based parallelism show good time to solution but for larger (**put plot) problem sizes particle-based parallelism pays off better. The finer/inner the thread forking the less significant scaling it is observed for larger problem sizes. For large problem sizes the combination of cell-based plus particle-based multi-core parallelism using hybrid-on-batches makes sense. Furthermore additional overall speedup can be gained if spheres are used as a filtering bounding box stage to our triangle-to-triangle contact detection.

\clearpage

